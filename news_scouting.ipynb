{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3356e4b0-9c0e-48c6-84d2-4f1761b115b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting news feed scanner...\n",
      "\n",
      "\n",
      "North America:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Americas... ‚úì (10 titles)\n",
      "  Fetching NPR News... ‚úì (10 titles)\n",
      "  Fetching The New York Times... ‚úì (10 titles)\n",
      "  Fetching CBC News Canada... ‚úì (10 titles)\n",
      "  Fetching The Guardian US... ‚úì (10 titles)\n",
      "  Fetching Mexico News Daily... ‚úì (10 titles)\n",
      "\n",
      "Europe:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Europe... ‚úì (10 titles)\n",
      "  Fetching The Guardian UK... ‚úì (10 titles)\n",
      "  Fetching Deutsche Welle English... ‚úì (10 titles)\n",
      "  Fetching France24 English... ‚úì (10 titles)\n",
      "  Fetching Euronews... ‚úì (10 titles)\n",
      "  Fetching POLITICO Europe... ‚úì (10 titles)\n",
      "  Fetching Swiss Info... ‚úì (0 titles)\n",
      "  Fetching Blick... ‚úì (4 titles)\n",
      "\n",
      "Asia:\n",
      "----------------------------------------\n",
      "  Fetching Al Jazeera... ‚úì (10 titles)\n",
      "  Fetching The Japan Times... ‚úì (10 titles)\n",
      "  Fetching South China Morning Post... ‚úì (10 titles)\n",
      "  Fetching The Hindu India... ‚úì (10 titles)\n",
      "  Fetching Times of India... ‚úì (10 titles)\n",
      "  Fetching Arab News... ‚úì (0 titles)\n",
      "\n",
      "Africa:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Africa... ‚úì (10 titles)\n",
      "  Fetching AllAfrica... ‚úì (10 titles)\n",
      "  Fetching Mail & Guardian SA... ‚úì (10 titles)\n",
      "  Fetching News24 South Africa... ‚úì (10 titles)\n",
      "  Fetching Morocco World News... ‚úì (10 titles)\n",
      "\n",
      "South America:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Latin America... ‚úì (10 titles)\n",
      "  Fetching Buenos Aires Times... ‚úì (10 titles)\n",
      "  Fetching MercoPress... ‚úì (0 titles)\n",
      "  Fetching Colombia Reports... ‚úì (10 titles)\n",
      "\n",
      "Oceania:\n",
      "----------------------------------------\n",
      "  Fetching ABC News Australia... ‚úì (10 titles)\n",
      "  Fetching Sydney Morning Herald... ‚úì (10 titles)\n",
      "  Fetching The Guardian Australia... ‚úì (10 titles)\n",
      "  Fetching Stuff.co.nz... ‚úì (10 titles)\n",
      "  Fetching Radio New Zealand... ‚úì (10 titles)\n",
      "\n",
      "\n",
      "Creating DataFrame...\n",
      "\n",
      "‚úÖ Success! Collected 304 articles from 31 sources\n",
      "\n",
      "Articles per continent:\n",
      "  North America: 60\n",
      "  Europe: 64\n",
      "  Asia: 50\n",
      "  Africa: 50\n",
      "  South America: 30\n",
      "  Oceania: 50\n",
      "\n",
      "üì∞ Sample headlines:\n",
      "================================================================================\n",
      "BBC News Americas: Why are these 12 countries on Trump's travel-ban list?...\n",
      "BBC News Americas: Trump's new ban dodges pitfalls faced by last attempt, experts say...\n",
      "BBC News Americas: Supreme Court rules for heterosexual woman in discrimination case...\n",
      "BBC News Americas: Supreme Court rejects Mexico lawsuit against US gunmakers...\n",
      "BBC News Americas: Trump suspends foreign student visas at Harvard...\n",
      "BBC News Americas: Trump orders inquiry into Biden's actions, alleging 'cognitive decline'...\n",
      "BBC News Americas: Musk turns on Republicans - and gives Trump's big bill a harder path...\n",
      "BBC News Americas: What Merz wants from Trump showdown meeting...\n",
      "BBC News Americas: Putin will seek revenge for Ukraine drone attack, warns Trump...\n",
      "BBC News Americas: Witness testifies that Diddy dangled her over apartment balcony...\n",
      "\n",
      "\n",
      "‚ú® DataFrame stored in variable 'news_df' with 304 articles\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict\n",
    "\n",
    "# News feeds organized by continent (excluding paid/API-required sources)\n",
    "NEWS_FEEDS = {\n",
    "    \"North America\": [\n",
    "        {\"name\": \"BBC News Americas\", \"url\": \"https://feeds.bbci.co.uk/news/world/us_and_canada/rss.xml\"},\n",
    "        {\"name\": \"NPR News\", \"url\": \"https://feeds.npr.org/1001/rss.xml\"},\n",
    "        {\"name\": \"The New York Times\", \"url\": \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\"},\n",
    "        {\"name\": \"CBC News Canada\", \"url\": \"https://rss.cbc.ca/lineup/topstories.xml\"},\n",
    "        {\"name\": \"The Guardian US\", \"url\": \"https://www.theguardian.com/us-news/rss\"},\n",
    "        {\"name\": \"Mexico News Daily\", \"url\": \"https://mexiconewsdaily.com/feed/\"}\n",
    "    ],\n",
    "    \"Europe\": [\n",
    "        {\"name\": \"BBC News Europe\", \"url\": \"https://feeds.bbci.co.uk/news/world/europe/rss.xml\"},\n",
    "        {\"name\": \"The Guardian UK\", \"url\": \"https://www.theguardian.com/uk/rss\"},\n",
    "        {\"name\": \"Deutsche Welle English\", \"url\": \"https://rss.dw.com/rdf/rss-en-all\"},\n",
    "        {\"name\": \"France24 English\", \"url\": \"https://www.france24.com/en/rss\"},\n",
    "        {\"name\": \"Euronews\", \"url\": \"https://www.euronews.com/rss\"},\n",
    "        {\"name\": \"POLITICO Europe\", \"url\": \"https://www.politico.eu/feed/\"},\n",
    "        {\"name\": \"Swiss Info\", \"url\": \"https://www.swissinfo.ch/eng/latest-news/rss\"},\n",
    "        {\"name\": \"Blick\", \"url\": \"https://www.blick.ch/news/rss.xml\"}\n",
    "    ],\n",
    "    \"Asia\": [\n",
    "        {\"name\": \"Al Jazeera\", \"url\": \"https://www.aljazeera.com/xml/rss/all.xml\"},\n",
    "        {\"name\": \"The Japan Times\", \"url\": \"https://www.japantimes.co.jp/feed/\"},\n",
    "        {\"name\": \"South China Morning Post\", \"url\": \"https://www.scmp.com/rss/91/feed\"},\n",
    "        {\"name\": \"The Hindu India\", \"url\": \"https://www.thehindu.com/news/national/feeder/default.rss\"},\n",
    "        {\"name\": \"Times of India\", \"url\": \"https://timesofindia.indiatimes.com/rssfeedstopstories.cms\"},\n",
    "        {\"name\": \"Arab News\", \"url\": \"https://www.arabnews.com/rss.xml\"}\n",
    "    ],\n",
    "    \"Africa\": [\n",
    "        {\"name\": \"BBC News Africa\", \"url\": \"https://feeds.bbci.co.uk/news/world/africa/rss.xml\"},\n",
    "        {\"name\": \"AllAfrica\", \"url\": \"https://allafrica.com/tools/headlines/rdf/latest/headlines.rdf\"},\n",
    "        {\"name\": \"Mail & Guardian SA\", \"url\": \"https://mg.co.za/feed/\"},\n",
    "        {\"name\": \"News24 South Africa\", \"url\": \"https://feeds.news24.com/articles/news24/TopStories/rss\"},\n",
    "        {\"name\": \"Morocco World News\", \"url\": \"https://www.moroccoworldnews.com/feed/\"}\n",
    "    ],\n",
    "    \"South America\": [\n",
    "        {\"name\": \"BBC News Latin America\", \"url\": \"https://feeds.bbci.co.uk/news/world/latin_america/rss.xml\"},\n",
    "        {\"name\": \"Buenos Aires Times\", \"url\": \"https://www.batimes.com.ar/feed\"},\n",
    "        {\"name\": \"MercoPress\", \"url\": \"https://en.mercopress.com/rss/v2/headlines\"},\n",
    "        {\"name\": \"Colombia Reports\", \"url\": \"https://colombiareports.com/feed/\"}\n",
    "    ],\n",
    "    \"Oceania\": [\n",
    "        {\"name\": \"ABC News Australia\", \"url\": \"https://www.abc.net.au/news/feed/2942460/rss.xml\"},\n",
    "        {\"name\": \"Sydney Morning Herald\", \"url\": \"https://www.smh.com.au/rss/feed.xml\"},\n",
    "        {\"name\": \"The Guardian Australia\", \"url\": \"https://www.theguardian.com/australia-news/rss\"},\n",
    "        {\"name\": \"Stuff.co.nz\", \"url\": \"https://www.stuff.co.nz/rss\"},\n",
    "        {\"name\": \"Radio New Zealand\", \"url\": \"https://www.rnz.co.nz/rss/national.xml\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "def fetch_feed_titles(feed_url, feed_name, max_titles=10):\n",
    "    \"\"\"Extract titles from a feed with simple error handling.\"\"\"\n",
    "    titles = []\n",
    "    \n",
    "    try:\n",
    "        # Fetch with timeout\n",
    "        print(f\"  Fetching {feed_name}...\", end=\"\", flush=True)\n",
    "        response = requests.get(feed_url, timeout=10, headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        \n",
    "        # Parse with feedparser\n",
    "        feed = feedparser.parse(response.content)\n",
    "        \n",
    "        # Extract titles\n",
    "        for i, entry in enumerate(feed.entries[:max_titles]):\n",
    "            if hasattr(entry, 'title'):\n",
    "                titles.append({\n",
    "                    'source': feed_name,\n",
    "                    'title': entry.title.strip(),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        print(f\" ‚úì ({len(titles)} titles)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" ‚úó (Error: {type(e).__name__})\")\n",
    "    \n",
    "    return titles\n",
    "\n",
    "# Main execution\n",
    "print(\"Starting news feed scanner...\\n\")\n",
    "all_results = []\n",
    "\n",
    "# Process each continent\n",
    "for continent, feeds in NEWS_FEEDS.items():\n",
    "    print(f\"\\n{continent}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for feed in feeds:\n",
    "        titles = fetch_feed_titles(feed['url'], feed['name'])\n",
    "        \n",
    "        # Add continent info to each title\n",
    "        for title in titles:\n",
    "            title['continent'] = continent\n",
    "            title['feed_url'] = feed['url']\n",
    "            all_results.append(title)\n",
    "        \n",
    "        # Small delay to be respectful\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Create DataFrame\n",
    "print(f\"\\n\\nCreating DataFrame...\")\n",
    "news_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Show summary\n",
    "if not news_df.empty:\n",
    "    print(f\"\\n‚úÖ Success! Collected {len(news_df)} articles from {news_df['source'].nunique()} sources\")\n",
    "    print(f\"\\nArticles per continent:\")\n",
    "    for continent in news_df['continent'].unique():\n",
    "        count = len(news_df[news_df['continent'] == continent])\n",
    "        print(f\"  {continent}: {count}\")\n",
    "    \n",
    "    print(\"\\nüì∞ Sample headlines:\")\n",
    "    print(\"=\" * 80)\n",
    "    for _, row in news_df.head(10).iterrows():\n",
    "        print(f\"{row['source']}: {row['title'][:100]}...\")\n",
    "else:\n",
    "    print(\"‚ùå No articles collected!\")\n",
    "\n",
    "# The DataFrame is now available as 'news_df'\n",
    "print(f\"\\n\\n‚ú® DataFrame stored in variable 'news_df' with {len(news_df)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bad33a84-373b-4095-92ef-48a1223ef8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded .env file\n",
      "üöÄ Starting 20min.ch content analysis...\n",
      "üìä DataFrame contains 304 articles\n",
      "Sending request to Claude API...\n",
      "Analyzing 304 articles from 31 sources...\n",
      "\n",
      "================================================================================\n",
      "CLAUDE'S RECOMMENDATIONS FOR 20MIN.CH\n",
      "================================================================================\n",
      "\n",
      "Here's my selection and analysis for 20min.ch readers:\n",
      "\n",
      "## TOP 5 NEWSWORTHY STORIES\n",
      "1. **BBC News**: \"What Merz wants from Trump showdown meeting\"\n",
      "   - **Why it matters to Swiss readers**: Direct Swiss connection with leading politician Merz meeting Trump, impacts Swiss-US relations\n",
      "   - **Key angle for 20min**: \"5 things at stake for Switzerland in the Merz-Trump meeting\"\n",
      "\n",
      "2. **Deutsche Welle**: \"ECB eyes end to cuts after trimming key interest rate to 2%\"\n",
      "   - **Why it matters to Swiss readers**: Direct impact on Swiss economy and mortgage rates\n",
      "   - **Key angle for 20min**: \"What the ECB's latest move means for your wallet\"\n",
      "\n",
      "3. **POLITICO Europe**: \"VPN signups surge after Pornhub pulls out of France\"\n",
      "   - **Why it matters to Swiss readers**: Similar age verification laws being discussed in Switzerland\n",
      "   - **Key angle for 20min**: \"Could Switzerland be next? French porn ban sparks debate\"\n",
      "\n",
      "4. **Blick**: \"Erfolgswelle: Schweizer Hotellerie mit √úbernachtungsrekord im Winter\"\n",
      "   - **Why it matters to Swiss readers**: Shows strength of local tourism industry\n",
      "   - **Key angle for 20min**: \"Swiss hotels break records: The surprising numbers behind the boom\"\n",
      "\n",
      "5. **BBC News**: \"Trump suspends foreign student visas at Harvard\"\n",
      "   - **Why it matters to Swiss readers**: Affects Swiss students planning to study in US\n",
      "   - **Key angle for 20min**: \"Swiss students caught in US visa freeze - what you need to know\"\n",
      "\n",
      "## TOP 5 TALKING PIECES\n",
      "1. **NPR News**: \"Divorce lawyers say it's a seasonal business. Here's why\"\n",
      "   - **Conversation starter**: Everyone knows someone who's divorced; timing aspect is surprising\n",
      "   - **20min angle**: \"Why spring is breakup season - Swiss relationship experts explain\"\n",
      "\n",
      "2. **BBC News Europe**: \"Pornhub pulls out of France over age verification law\"\n",
      "   - **Conversation starter**: Controversial topic mixing tech, privacy and social issues\n",
      "   - **20min angle**: \"French teens flood Swiss servers after Pornhub ban\"\n",
      "\n",
      "3. **BBC News Europe**: \"Eight injured on Ryanair flight hit by 'severe turbulence'\"\n",
      "   - **Conversation starter**: Many readers fly Ryanair from Swiss airports\n",
      "   - **20min angle**: \"Horror flight: Swiss passenger describes chaos aboard Ryanair plane\"\n",
      "\n",
      "4. **The Guardian UK**: \"'Do you have a family?': midlife with no kids, ageing parents ‚Äì and no crisis\"\n",
      "   - **Conversation starter**: Relevant to many young urban professionals\n",
      "   - **20min angle**: \"Child-free by choice: Young Swiss share their stories\"\n",
      "\n",
      "5. **South China Morning Post**: \"China will drop the Great Firewall for some users\"\n",
      "   - **Conversation starter**: Surprising development from usually restrictive China\n",
      "   - **20min angle**: \"China's internet experiment: What Swiss tourists need to know\"\n",
      "\n",
      "All stories selected considering:\n",
      "- Swiss relevance or easy localization\n",
      "- Appeal to young urban audience\n",
      "- Social media shareability\n",
      "- Potential for reader engagement\n",
      "- Visual storytelling opportunities\n",
      "- Mix of serious news and lighter topics\n",
      "\n",
      "‚úÖ Analysis complete! Check the output above for Claude's recommendations.\n",
      "\n",
      "Tip: You can copy the recommended titles and search for them in your original DataFrame:\n",
      "Example: news_df[news_df['title'].str.contains('search_term', case=False)]\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables with dotenv if available (for local development)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"‚úÖ Loaded .env file\")\n",
    "except ImportError:\n",
    "    print(\"üìù dotenv not available - using system environment variables\")\n",
    "\n",
    "# Initialize the Claude client with your API key\n",
    "API_KEY = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "# Add error checking\n",
    "if not API_KEY:\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY environment variable not set!\")\n",
    "\n",
    "client = anthropic.Anthropic(api_key=API_KEY)\n",
    "\n",
    "# Craft the prompt based on 20min.ch's audience profile\n",
    "def create_prompt(df_json):\n",
    "    prompt = f\"\"\"You are a news editor for 20min.ch, Switzerland's leading free commuter tabloid with over 2 million readers. Your audience consists of:\n",
    "\n",
    "**TARGET AUDIENCE PROFILE:**\n",
    "- Young urban commuters (15-40 years old)\n",
    "- German-speaking Swiss citizens\n",
    "- Quick readers who consume news during their commute (average 20 minutes)\n",
    "- Prefer bite-sized, engaging content with visual appeal\n",
    "- Interested in local Swiss news, lifestyle, entertainment, and conversational stories\n",
    "- Enjoy interactive content, social media integration, and stories that spark discussion\n",
    "- Appreciate humor, human interest stories, and relatable content\n",
    "\n",
    "**YOUR TASK:**\n",
    "Analyze the following news articles from various international sources and identify the most suitable content for 20min.ch readers.\n",
    "\n",
    "**SELECTION CRITERIA:**\n",
    "1. **Newsworthy Stories**: Must be genuinely important, impact Swiss readers, or have global significance\n",
    "2. **Talking Pieces**: Stories that will generate conversation, debate, or emotional response among young Swiss readers\n",
    "\n",
    "Consider:\n",
    "- Swiss relevance or connection\n",
    "- Shareability on social media\n",
    "- Visual story potential\n",
    "- Emotional impact (surprising, funny, shocking, heartwarming)\n",
    "- Relatability to young urban lifestyle\n",
    "- Potential for reader engagement/comments\n",
    "\n",
    "**NEWS DATA:**\n",
    "{df_json}\n",
    "\n",
    "**REQUIRED OUTPUT:**\n",
    "Please provide exactly 10 stories in the following format:\n",
    "\n",
    "## TOP 5 NEWSWORTHY STORIES\n",
    "1. **[Original Source]**: [Title]\n",
    "   - **Why it matters to Swiss readers**: [Brief explanation]\n",
    "   - **Key angle for 20min**: [How to present it]\n",
    "\n",
    "2-5. [Continue same format]\n",
    "\n",
    "## TOP 5 TALKING PIECES\n",
    "1. **[Original Source]**: [Title]\n",
    "   - **Conversation starter**: [What makes this discussable]\n",
    "   - **20min angle**: [How to make it engaging]\n",
    "\n",
    "2-5. [Continue same format]\n",
    "\n",
    "Remember: 20min.ch readers want quick, impactful stories they can discuss with friends or share on social media. Focus on human stories, surprising facts, and content that connects to Swiss life.\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Function to make the API call\n",
    "def analyze_news_for_20min(news_df):\n",
    "    \"\"\"\n",
    "    Send news DataFrame to Claude API and get recommendations for 20min.ch\n",
    "    \"\"\"\n",
    "    # Convert DataFrame to JSON for the prompt\n",
    "    df_json = news_df.to_json(orient='records', indent=2)\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = create_prompt(df_json)\n",
    "    \n",
    "    print(\"Sending request to Claude API...\")\n",
    "    print(f\"Analyzing {len(news_df)} articles from {news_df['source'].nunique()} sources...\")\n",
    "    \n",
    "    try:\n",
    "        # Make the API call\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",  # Using Sonnet 3.5\n",
    "            max_tokens=2000,\n",
    "            temperature=0.7,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract the response\n",
    "        analysis = response.content[0].text\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CLAUDE'S RECOMMENDATIONS FOR 20MIN.CH\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        print(analysis)\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error calling Claude API: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run the analysis\n",
    "print(\"üöÄ Starting 20min.ch content analysis...\")\n",
    "print(f\"üìä DataFrame contains {len(news_df)} articles\")\n",
    "\n",
    "# Call the function with your news DataFrame\n",
    "analysis_result = analyze_news_for_20min(news_df)\n",
    "\n",
    "# Optional: Create a summary DataFrame of recommended articles\n",
    "if analysis_result:\n",
    "    print(\"\\n‚úÖ Analysis complete! Check the output above for Claude's recommendations.\")\n",
    "    print(\"\\nTip: You can copy the recommended titles and search for them in your original DataFrame:\")\n",
    "    print(\"Example: news_df[news_df['title'].str.contains('search_term', case=False)]\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "27deb46d-1c85-4ec1-ba9b-5a79482d0769",
   "metadata": {},
   "outputs": [],
   "source": "import requests\nimport json\nimport os\nfrom datetime import datetime\n\n# Load environment variables with dotenv if available (for local development)\ntry:\n    from dotenv import load_dotenv\n    load_dotenv()\n    print(\"‚úÖ Loaded .env file\")\nexcept ImportError:\n    print(\"üìù dotenv not available - using system environment variables\")\n\n# Your Zapier webhook URL\nZAPIER_WEBHOOK_URL = os.environ.get('ZAPIER_WEBHOOK_URL')\n\n# Add error checking\nif not ZAPIER_WEBHOOK_URL:\n    raise ValueError(\"ZAPIER_WEBHOOK_URL environment variable not set!\")\n\ndef format_for_google_docs(analysis_text, news_df):\n    \"\"\"\n    Format the analysis as clean, compact HTML for Google Docs in German\n    \"\"\"\n    # Start with a simple, compact header in German\n    html_content = f\"\"\"<h1>20MIN.CH T√ÑGLICHE NEWS-ANALYSE</h1>\n<h2>{datetime.now().strftime('%d. %B %Y')}</h2>\n\n<h3>ANALYSE-ZUSAMMENFASSUNG</h3>\n<p>\n<strong>Erstellt:</strong> {datetime.now().strftime('%d.%m.%Y um %H:%M CET')}<br>\n<strong>Analysierte Artikel:</strong> {len(news_df)}<br>\n<strong>Nachrichtenquellen:</strong> {news_df['source'].nunique()}<br>\n<strong>Kontinente abgedeckt:</strong> {news_df['continent'].nunique()}\n</p>\n\n<hr>\n\"\"\"\n    \n    # Process the analysis text with minimal formatting\n    lines = analysis_text.split('\\n')\n    \n    for line in lines:\n        line = line.strip()\n        \n        if not line:\n            continue\n            \n        # Handle main headers\n        if line.startswith('## '):\n            header = line.replace('## ', '').replace('**', '')\n            html_content += f'<h2>{header}</h2>\\n'\n        \n        # Handle story entries (numbered items) - keep original titles without translation\n        elif line[0:2] in ['1.', '2.', '3.', '4.', '5.']:\n            # Parse the story line\n            parts = line.split('**')\n            if len(parts) >= 3:\n                number = parts[0].strip()\n                source = parts[1].strip()\n                title = parts[2].strip().lstrip(':').strip()\n                \n                # Simple, compact formatting - keep original title\n                html_content += f'<p><strong>{number} {source}:</strong> {title}</p>\\n'\n        \n        # Handle bullet points with German labels\n        elif line.startswith('- '):\n            content = line[2:].strip()\n            \n            # Handle bold markers\n            if '**' in content:\n                parts = content.split('**')\n                formatted_content = \"\"\n                for i, part in enumerate(parts):\n                    if i % 2 == 1:\n                        formatted_content += f'<strong>{part}</strong>'\n                    else:\n                        formatted_content += part\n                content = formatted_content\n            \n            # Translate common English labels to German\n            content = content.replace('Why it matters to Swiss readers:', 'Warum es f√ºr Schweizer Leser wichtig ist:')\n            content = content.replace('Key angle for 20min:', '20min-Winkel:')\n            content = content.replace('Conversation starter:', 'Gespr√§chsansto√ü:')\n            content = content.replace('20min angle:', '20min-Winkel:')\n            \n            # Add as indented paragraph\n            if ':' in content:\n                label, value = content.split(':', 1)\n                html_content += f'<p style=\"margin-left: 20px;\">‚Ä¢ <strong>{label}:</strong>{value}</p>\\n'\n            else:\n                html_content += f'<p style=\"margin-left: 20px;\">‚Ä¢ {content}</p>\\n'\n    \n    # Add compact methodology section in German\n    html_content += f\"\"\"\n<hr>\n<h3>METHODIK</h3>\n<p>Diese Analyse wurde erstellt durch:</p>\n<p style=\"margin-left: 20px;\">\n‚Ä¢ Scannen von RSS-Feeds gro√üer Nachrichtenportale aller Kontinente<br>\n‚Ä¢ Sammeln der neuesten {len(news_df)} Artikel von {news_df['source'].nunique()} Quellen<br>\n‚Ä¢ Verwendung von Claude AI zur Identifikation der f√ºr 20min.ch-Leser relevantesten Stories<br>\n‚Ä¢ Anwendung von Auswahlkriterien: Schweiz-Relevanz, Teilbarkeit, visuelles Potenzial, emotionale Wirkung, Gespr√§chspotenzial\n</p>\n\n<h3>QUELLENVERTEILUNG</h3>\n<p>\"\"\"\n    \n    # Add source statistics in a compact format with German continent names\n    continent_names_de = {\n        'North America': 'Nordamerika',\n        'Europe': 'Europa', \n        'Asia': 'Asien',\n        'Africa': 'Afrika',\n        'South America': 'S√ºdamerika',\n        'Oceania': 'Ozeanien'\n    }\n    \n    source_lines = []\n    for continent in news_df['continent'].value_counts().index:\n        count = len(news_df[news_df['continent'] == continent])\n        continent_de = continent_names_de.get(continent, continent)\n        source_lines.append(f'<strong>{continent_de}:</strong> {count} Artikel')\n    \n    html_content += ' | '.join(source_lines)\n    \n    html_content += f\"\"\"\n</p>\n\n<hr>\n<p><em>Automatisch generiert via GitHub Actions um {datetime.now().strftime('%H:%M:%S CET')}</em></p>\n\"\"\"\n    \n    return html_content\n\ndef create_email_html(analysis_text, news_df):\n    \"\"\"\n    Create HTML content for the email with summary in German\n    \"\"\"\n    # Extract top stories for email preview\n    stories_preview = extract_top_stories(analysis_text)\n    \n    html_content = f\"\"\"\n<div style=\"font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto;\">\n    <h2 style=\"color: #d32f2f;\">20min.ch T√§gliche News-Analyse</h2>\n    \n    <p>Hallo Tom,</p>\n    \n    <p>Deine automatisierte News-Analyse f√ºr <strong>{datetime.now().strftime('%d.%m.%Y')}</strong> ist bereit.</p>\n    \n    <div style=\"background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin: 20px 0;\">\n        <h3 style=\"margin-top: 0;\">Schnell√ºbersicht:</h3>\n        <ul style=\"list-style-type: none; padding-left: 0;\">\n            <li>üìä Analysierte Artikel: {len(news_df)}</li>\n            <li>üì∞ Nachrichtenquellen: {news_df['source'].nunique()}</li>\n            <li>üåç Kontinente abgedeckt: {news_df['continent'].nunique()}</li>\n            <li>‚è∞ Erstellt um: {datetime.now().strftime('%H:%M CET')}</li>\n        </ul>\n    </div>\n    \n    <div style=\"margin: 20px 0;\">\n        <h3>Heutige Empfehlungen:</h3>\n        {stories_preview}\n    </div>\n    \n    <p style=\"margin-top: 30px;\">\n        <em>üí° Die vollst√§ndige Analyse enth√§lt detaillierte Empfehlungen f√ºr jede Story, \n        einschlie√ülich Schweizer Relevanz und vorgeschlagener Pr√§sentationswinkel.</em>\n    </p>\n    \n    <hr style=\"border: none; border-top: 1px solid #ddd; margin: 30px 0;\">\n    \n    <p style=\"color: #666; font-size: 12px;\">\n        Diese Analyse wurde automatisch mit KI erstellt, um die f√ºr das 20min.ch-Publikum \n        relevantesten Stories zu identifizieren.\n    </p>\n</div>\n\"\"\"\n    \n    return html_content\n\ndef extract_top_stories(analysis_text):\n    \"\"\"\n    Extract all 5 stories from each category for email preview\n    \"\"\"\n    preview_html = \"\"\n    lines = analysis_text.split('\\n')\n    \n    # Flags to track sections\n    in_newsworthy = False\n    in_talking = False\n    story_count = 0\n    \n    for line in lines:\n        if \"TOP 5 NEWSWORTHY\" in line:\n            in_newsworthy = True\n            in_talking = False\n            story_count = 0\n            preview_html += \"<h4>üî• Top 5 Newsworthy Stories:</h4><ul>\"\n        elif \"TOP 5 TALKING\" in line:\n            in_newsworthy = False\n            in_talking = True\n            story_count = 0\n            if \"</ul>\" not in preview_html[-5:]:\n                preview_html += \"</ul>\"\n            preview_html += \"<h4>üí¨ Top 5 Talking Pieces:</h4><ul>\"\n        elif (in_newsworthy or in_talking) and story_count < 5:\n            if line.strip().startswith(('1.', '2.', '3.', '4.', '5.')):\n                # Extract the title part\n                if '**' in line and ':' in line:\n                    parts = line.split('**')\n                    if len(parts) >= 3:\n                        source = parts[1]\n                        title = parts[2].split(':')[1].strip() if ':' in parts[2] else parts[2].strip()\n                        preview_html += f\"<li><strong>{source}</strong>: {title}</li>\"\n                        story_count += 1\n    \n    if \"</ul>\" not in preview_html[-5:]:\n        preview_html += \"</ul>\"\n    \n    return preview_html\n\ndef send_to_zapier(analysis_text, news_df):\n    \"\"\"\n    Send the formatted data to Zapier webhook\n    \"\"\"\n    print(\"üì§ Sending data to Zapier...\")\n    \n    # Prepare the payload\n    payload = {\n        \"date\": datetime.now().strftime(\"%d.%m.%Y\"),\n        \"time\": datetime.now().strftime(\"%H:%M CET\"),\n        \"document_title\": f\"20min.ch News-Analyse - {datetime.now().strftime('%d.%m.%Y')}\",\n        \"document_content\": format_for_google_docs(analysis_text, news_df),\n        \"email_content_html\": create_email_html(analysis_text, news_df),\n        \"stats\": {\n            \"total_articles\": len(news_df),\n            \"total_sources\": news_df['source'].nunique(),\n            \"continents\": news_df['continent'].nunique()\n        },\n        \"recipient_email\": \"tom.vaillant@20minuten.ch\",\n        \"email_subject\": f\"T√§gliche News-Analyse - {datetime.now().strftime('%d.%m.%Y')}\"\n    }\n    \n    try:\n        # Send to Zapier\n        response = requests.post(ZAPIER_WEBHOOK_URL, json=payload)\n        \n        if response.status_code == 200:\n            print(\"‚úÖ Successfully sent to Zapier!\")\n            print(f\"üìß Email will be sent to: {payload['recipient_email']}\")\n            print(f\"üìÑ Document title: {payload['document_title']}\")\n            print(\"\\nüéØ Next steps in Zapier:\")\n            print(\"1. Google Doc will be created automatically\")\n            print(\"2. Email will be sent with the doc link\")\n            return True\n        else:\n            print(f\"‚ùå Error sending to Zapier: {response.status_code}\")\n            print(f\"Response: {response.text}\")\n            return False\n            \n    except Exception as e:\n        print(f\"‚ùå Exception occurred: {str(e)}\")\n        return False\n\n# Test the webhook with sample data (optional)\ndef test_zapier_webhook():\n    \"\"\"\n    Test the Zapier webhook with minimal data\n    \"\"\"\n    test_payload = {\n        \"test\": True,\n        \"message\": \"Testing webhook connection\",\n        \"timestamp\": datetime.now().isoformat()\n    }\n    \n    print(\"üß™ Testing Zapier webhook...\")\n    response = requests.post(ZAPIER_WEBHOOK_URL, json=test_payload)\n    \n    if response.status_code == 200:\n        print(\"‚úÖ Webhook test successful!\")\n        print(\"Check your Zapier dashboard to see the test data\")\n    else:\n        print(f\"‚ùå Webhook test failed: {response.status_code}\")\n\n# Main execution for your Jupyter notebook\nif 'analysis_result' in globals() and 'news_df' in globals():\n    # Send the analysis to Zapier\n    success = send_to_zapier(analysis_result, news_df)\n    \n    if success:\n        print(\"\\nüéâ All done! Check your Zapier dashboard to monitor the workflow.\")\nelse:\n    print(\"‚ùå No analysis results found. Please run the Claude analysis first.\")\n    print(\"\\nüí° To test your webhook connection, run:\")\n    print(\"test_zapier_webhook()\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed19cd-ef91-479f-a1c3-47e36f494080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}