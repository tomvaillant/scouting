{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3356e4b0-9c0e-48c6-84d2-4f1761b115b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting news feed scanner...\n",
      "\n",
      "\n",
      "North America:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Americas... ‚úì (5 titles)\n",
      "  Fetching NPR News... ‚úì (5 titles)\n",
      "  Fetching The New York Times... ‚úì (5 titles)\n",
      "  Fetching CBC News Canada... ‚úì (5 titles)\n",
      "  Fetching The Guardian US... ‚úì (5 titles)\n",
      "  Fetching Mexico News Daily... ‚úì (5 titles)\n",
      "\n",
      "Europe:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Europe... ‚úì (5 titles)\n",
      "  Fetching The Guardian UK... ‚úì (5 titles)\n",
      "  Fetching Deutsche Welle English... ‚úì (5 titles)\n",
      "  Fetching France24 English... ‚úì (5 titles)\n",
      "  Fetching Euronews... ‚úì (5 titles)\n",
      "  Fetching POLITICO Europe... ‚úì (5 titles)\n",
      "  Fetching Swiss Info... ‚úì (0 titles)\n",
      "\n",
      "Asia:\n",
      "----------------------------------------\n",
      "  Fetching Al Jazeera... ‚úì (5 titles)\n",
      "  Fetching The Japan Times... ‚úì (5 titles)\n",
      "  Fetching South China Morning Post... ‚úì (5 titles)\n",
      "  Fetching The Hindu India... ‚úì (5 titles)\n",
      "  Fetching Times of India... ‚úì (5 titles)\n",
      "  Fetching Arab News... ‚úì (5 titles)\n",
      "\n",
      "Africa:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Africa... ‚úì (5 titles)\n",
      "  Fetching AllAfrica... ‚úì (5 titles)\n",
      "  Fetching Mail & Guardian SA... ‚úì (5 titles)\n",
      "  Fetching News24 South Africa... ‚úì (5 titles)\n",
      "  Fetching Morocco World News... ‚úì (5 titles)\n",
      "\n",
      "South America:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Latin America... ‚úì (5 titles)\n",
      "  Fetching Buenos Aires Times... ‚úì (5 titles)\n",
      "  Fetching MercoPress... ‚úì (0 titles)\n",
      "  Fetching Colombia Reports... ‚úì (5 titles)\n",
      "\n",
      "Oceania:\n",
      "----------------------------------------\n",
      "  Fetching ABC News Australia... ‚úì (5 titles)\n",
      "  Fetching Sydney Morning Herald... ‚úì (5 titles)\n",
      "  Fetching The Guardian Australia... ‚úì (5 titles)\n",
      "  Fetching Stuff.co.nz... ‚úì (5 titles)\n",
      "  Fetching Radio New Zealand... ‚úì (5 titles)\n",
      "\n",
      "\n",
      "Creating DataFrame...\n",
      "\n",
      "‚úÖ Success! Collected 155 articles from 31 sources\n",
      "\n",
      "Articles per continent:\n",
      "  North America: 30\n",
      "  Europe: 30\n",
      "  Asia: 30\n",
      "  Africa: 25\n",
      "  South America: 15\n",
      "  Oceania: 25\n",
      "\n",
      "üíæ Saved to news_scan_20250605_163256.csv\n",
      "\n",
      "üì∞ Sample headlines:\n",
      "================================================================================\n",
      "BBC News Americas: What we know about Trump's latest travel ban...\n",
      "BBC News Americas: Why are these 12 countries on Trump's travel-ban list?...\n",
      "BBC News Americas: Ten African countries hit by bans and restrictions...\n",
      "BBC News Americas: Trump suspends foreign student visas at Harvard...\n",
      "BBC News Americas: Trump orders inquiry into Biden's actions, alleging 'cognitive decline'...\n",
      "NPR News: China says Trump and Xi talked on the phone, their first call since the tariff war began...\n",
      "NPR News: Israel says it recovered the bodies of 2 U.S.-Israeli hostages...\n",
      "NPR News: Trump issues a new travel ban. And, GOP raises concerns over the budget bill...\n",
      "NPR News: FEMA was starting to fix long-standing problems. Then came the Trump administration...\n",
      "NPR News: Divorce lawyers say it's a seasonal business. Here's why...\n",
      "\n",
      "\n",
      "‚ú® DataFrame stored in variable 'news_df' with 155 articles\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict\n",
    "\n",
    "# News feeds organized by continent (excluding paid/API-required sources)\n",
    "NEWS_FEEDS = {\n",
    "    \"North America\": [\n",
    "        {\"name\": \"BBC News Americas\", \"url\": \"https://feeds.bbci.co.uk/news/world/us_and_canada/rss.xml\"},\n",
    "        {\"name\": \"NPR News\", \"url\": \"https://feeds.npr.org/1001/rss.xml\"},\n",
    "        {\"name\": \"The New York Times\", \"url\": \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\"},\n",
    "        {\"name\": \"CBC News Canada\", \"url\": \"https://rss.cbc.ca/lineup/topstories.xml\"},\n",
    "        {\"name\": \"The Guardian US\", \"url\": \"https://www.theguardian.com/us-news/rss\"},\n",
    "        {\"name\": \"Mexico News Daily\", \"url\": \"https://mexiconewsdaily.com/feed/\"}\n",
    "    ],\n",
    "    \"Europe\": [\n",
    "        {\"name\": \"BBC News Europe\", \"url\": \"https://feeds.bbci.co.uk/news/world/europe/rss.xml\"},\n",
    "        {\"name\": \"The Guardian UK\", \"url\": \"https://www.theguardian.com/uk/rss\"},\n",
    "        {\"name\": \"Deutsche Welle English\", \"url\": \"https://rss.dw.com/rdf/rss-en-all\"},\n",
    "        {\"name\": \"France24 English\", \"url\": \"https://www.france24.com/en/rss\"},\n",
    "        {\"name\": \"Euronews\", \"url\": \"https://www.euronews.com/rss\"},\n",
    "        {\"name\": \"POLITICO Europe\", \"url\": \"https://www.politico.eu/feed/\"},\n",
    "        {\"name\": \"Swiss Info\", \"url\": \"https://www.swissinfo.ch/eng/latest-news/rss\"}\n",
    "    ],\n",
    "    \"Asia\": [\n",
    "        {\"name\": \"Al Jazeera\", \"url\": \"https://www.aljazeera.com/xml/rss/all.xml\"},\n",
    "        {\"name\": \"The Japan Times\", \"url\": \"https://www.japantimes.co.jp/feed/\"},\n",
    "        {\"name\": \"South China Morning Post\", \"url\": \"https://www.scmp.com/rss/91/feed\"},\n",
    "        {\"name\": \"The Hindu India\", \"url\": \"https://www.thehindu.com/news/national/feeder/default.rss\"},\n",
    "        {\"name\": \"Times of India\", \"url\": \"https://timesofindia.indiatimes.com/rssfeedstopstories.cms\"},\n",
    "        {\"name\": \"Arab News\", \"url\": \"https://www.arabnews.com/rss.xml\"}\n",
    "    ],\n",
    "    \"Africa\": [\n",
    "        {\"name\": \"BBC News Africa\", \"url\": \"https://feeds.bbci.co.uk/news/world/africa/rss.xml\"},\n",
    "        {\"name\": \"AllAfrica\", \"url\": \"https://allafrica.com/tools/headlines/rdf/latest/headlines.rdf\"},\n",
    "        {\"name\": \"Mail & Guardian SA\", \"url\": \"https://mg.co.za/feed/\"},\n",
    "        {\"name\": \"News24 South Africa\", \"url\": \"https://feeds.news24.com/articles/news24/TopStories/rss\"},\n",
    "        {\"name\": \"Morocco World News\", \"url\": \"https://www.moroccoworldnews.com/feed/\"}\n",
    "    ],\n",
    "    \"South America\": [\n",
    "        {\"name\": \"BBC News Latin America\", \"url\": \"https://feeds.bbci.co.uk/news/world/latin_america/rss.xml\"},\n",
    "        {\"name\": \"Buenos Aires Times\", \"url\": \"https://www.batimes.com.ar/feed\"},\n",
    "        {\"name\": \"MercoPress\", \"url\": \"https://en.mercopress.com/rss/v2/headlines\"},\n",
    "        {\"name\": \"Colombia Reports\", \"url\": \"https://colombiareports.com/feed/\"}\n",
    "    ],\n",
    "    \"Oceania\": [\n",
    "        {\"name\": \"ABC News Australia\", \"url\": \"https://www.abc.net.au/news/feed/2942460/rss.xml\"},\n",
    "        {\"name\": \"Sydney Morning Herald\", \"url\": \"https://www.smh.com.au/rss/feed.xml\"},\n",
    "        {\"name\": \"The Guardian Australia\", \"url\": \"https://www.theguardian.com/australia-news/rss\"},\n",
    "        {\"name\": \"Stuff.co.nz\", \"url\": \"https://www.stuff.co.nz/rss\"},\n",
    "        {\"name\": \"Radio New Zealand\", \"url\": \"https://www.rnz.co.nz/rss/national.xml\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "def fetch_feed_titles(feed_url, feed_name, max_titles=5):\n",
    "    \"\"\"Extract titles from a feed with simple error handling.\"\"\"\n",
    "    titles = []\n",
    "    \n",
    "    try:\n",
    "        # Fetch with timeout\n",
    "        print(f\"  Fetching {feed_name}...\", end=\"\", flush=True)\n",
    "        response = requests.get(feed_url, timeout=10, headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        \n",
    "        # Parse with feedparser\n",
    "        feed = feedparser.parse(response.content)\n",
    "        \n",
    "        # Extract titles\n",
    "        for i, entry in enumerate(feed.entries[:max_titles]):\n",
    "            if hasattr(entry, 'title'):\n",
    "                titles.append({\n",
    "                    'source': feed_name,\n",
    "                    'title': entry.title.strip(),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        print(f\" ‚úì ({len(titles)} titles)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" ‚úó (Error: {type(e).__name__})\")\n",
    "    \n",
    "    return titles\n",
    "\n",
    "# Main execution\n",
    "print(\"Starting news feed scanner...\\n\")\n",
    "all_results = []\n",
    "\n",
    "# Process each continent\n",
    "for continent, feeds in NEWS_FEEDS.items():\n",
    "    print(f\"\\n{continent}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for feed in feeds:\n",
    "        titles = fetch_feed_titles(feed['url'], feed['name'])\n",
    "        \n",
    "        # Add continent info to each title\n",
    "        for title in titles:\n",
    "            title['continent'] = continent\n",
    "            title['feed_url'] = feed['url']\n",
    "            all_results.append(title)\n",
    "        \n",
    "        # Small delay to be respectful\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Create DataFrame\n",
    "print(f\"\\n\\nCreating DataFrame...\")\n",
    "news_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Show summary\n",
    "if not news_df.empty:\n",
    "    print(f\"\\n‚úÖ Success! Collected {len(news_df)} articles from {news_df['source'].nunique()} sources\")\n",
    "    print(f\"\\nArticles per continent:\")\n",
    "    for continent in news_df['continent'].unique():\n",
    "        count = len(news_df[news_df['continent'] == continent])\n",
    "        print(f\"  {continent}: {count}\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    filename = f\"news_scan_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    news_df.to_csv(filename, index=False)\n",
    "    print(f\"\\nüíæ Saved to {filename}\")\n",
    "    \n",
    "    print(\"\\nüì∞ Sample headlines:\")\n",
    "    print(\"=\" * 80)\n",
    "    for _, row in news_df.head(10).iterrows():\n",
    "        print(f\"{row['source']}: {row['title'][:100]}...\")\n",
    "else:\n",
    "    print(\"‚ùå No articles collected!\")\n",
    "\n",
    "# The DataFrame is now available as 'news_df'\n",
    "print(f\"\\n\\n‚ú® DataFrame stored in variable 'news_df' with {len(news_df)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bad33a84-373b-4095-92ef-48a1223ef8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting 20min.ch content analysis...\n",
      "üìä DataFrame contains 155 articles\n",
      "Sending request to Claude API...\n",
      "Analyzing 155 articles from 31 sources...\n",
      "\n",
      "================================================================================\n",
      "CLAUDE'S RECOMMENDATIONS FOR 20MIN.CH\n",
      "================================================================================\n",
      "\n",
      "Here's my selection and analysis for 20min.ch readers:\n",
      "\n",
      "## TOP 5 NEWSWORTHY STORIES\n",
      "1. **BBC News Europe**: \"Eight injured on Ryanair flight hit by 'severe turbulence'\"\n",
      "   - **Why it matters to Swiss readers**: Aviation safety directly impacts Swiss travelers who frequently use low-cost carriers\n",
      "   - **Key angle for 20min**: Focus on passenger experiences and safety tips for budget airline travel\n",
      "\n",
      "2. **Deutsche Welle English**: \"Germany updates: Merz in Washington for talks with Trump\"\n",
      "   - **Why it matters to Swiss readers**: Germany's relationship with Trump could affect Swiss-EU-US relations\n",
      "   - **Key angle for 20min**: What Trump's potential return means for Switzerland's key trading partner\n",
      "\n",
      "3. **POLITICO Europe**: \"Lagarde insists she'll complete her term at ECB\"\n",
      "   - **Why it matters to Swiss readers**: ECB decisions directly impact Swiss franc and economy\n",
      "   - **Key angle for 20min**: How ECB stability affects Swiss mortgages and savings\n",
      "\n",
      "4. **BBC News Europe**: \"Putin will seek revenge for Ukraine drone attack, warns Trump\"\n",
      "   - **Why it matters to Swiss readers**: Escalation could affect Swiss neutrality and regional stability\n",
      "   - **Key angle for 20min**: Switzerland's role as neutral mediator in increasing tensions\n",
      "\n",
      "5. **South China Morning Post**: \"China will drop the Great Firewall for some users, but only in southern Hainan\"\n",
      "   - **Why it matters to Swiss readers**: Impact on Swiss businesses operating in China\n",
      "   - **Key angle for 20min**: What this means for Swiss companies expanding to China\n",
      "\n",
      "## TOP 5 TALKING PIECES\n",
      "1. **The Guardian UK**: \"I thought it was being gay that made my life so difficult. Then, at 50, I got an eye-opening diagnosis...\"\n",
      "   - **Conversation starter**: Late-life diagnoses and identity struggles\n",
      "   - **20min angle**: Personal story format with Swiss expert commentary on adult diagnosis rates\n",
      "\n",
      "2. **NPR News**: \"Divorce lawyers say it's a seasonal business. Here's why\"\n",
      "   - **Conversation starter**: Surprising patterns in relationship breakdowns\n",
      "   - **20min angle**: Compare with Swiss divorce statistics and seasonal patterns\n",
      "\n",
      "3. **Times of India**: \"AI killing jobs: Google AI CEO Hassabis has a tip; If I were a student..\"\n",
      "   - **Conversation starter**: Career choices in the AI era\n",
      "   - **20min angle**: Swiss job market perspective and advice from local tech leaders\n",
      "\n",
      "4. **News24 South Africa**: \"Three Cape Town restaurants make the World's Best list for 2025\"\n",
      "   - **Conversation starter**: Global food scene and Swiss culinary standing\n",
      "   - **20min angle**: Compare with Swiss restaurants' rankings and local food trends\n",
      "\n",
      "5. **Euronews**: \"More than 200 items from Princess Diana's wardrobe go up for auction\"\n",
      "   - **Conversation starter**: Celebrity memorabilia and fashion history\n",
      "   - **20min angle**: Swiss auction houses' perspective and local collector interest\n",
      "\n",
      "Each story has been selected for its relevance to Swiss readers and potential for engagement on social media platforms popular with 20min.ch's young urban audience.\n",
      "\n",
      "üíæ Analysis saved to: 20min_analysis_20250605_164028.txt\n",
      "\n",
      "‚úÖ Analysis complete! Check the output above for Claude's recommendations.\n",
      "\n",
      "Tip: You can copy the recommended titles and search for them in your original DataFrame:\n",
      "Example: news_df[news_df['title'].str.contains('search_term', case=False)]\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize the Claude client with your API key\n",
    "API_KEY = \"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\"\n",
    "client = anthropic.Anthropic(api_key=API_KEY)\n",
    "\n",
    "# Craft the prompt based on 20min.ch's audience profile\n",
    "def create_prompt(df_json):\n",
    "    prompt = f\"\"\"You are a news editor for 20min.ch, Switzerland's leading free commuter tabloid with over 2 million readers. Your audience consists of:\n",
    "\n",
    "**TARGET AUDIENCE PROFILE:**\n",
    "- Young urban commuters (15-40 years old)\n",
    "- German-speaking Swiss citizens\n",
    "- Quick readers who consume news during their commute (average 20 minutes)\n",
    "- Prefer bite-sized, engaging content with visual appeal\n",
    "- Interested in local Swiss news, lifestyle, entertainment, and conversational stories\n",
    "- Enjoy interactive content, social media integration, and stories that spark discussion\n",
    "- Appreciate humor, human interest stories, and relatable content\n",
    "\n",
    "**YOUR TASK:**\n",
    "Analyze the following news articles from various international sources and identify the most suitable content for 20min.ch readers.\n",
    "\n",
    "**SELECTION CRITERIA:**\n",
    "1. **Newsworthy Stories**: Must be genuinely important, impact Swiss readers, or have global significance\n",
    "2. **Talking Pieces**: Stories that will generate conversation, debate, or emotional response among young Swiss readers\n",
    "\n",
    "Consider:\n",
    "- Swiss relevance or connection\n",
    "- Shareability on social media\n",
    "- Visual story potential\n",
    "- Emotional impact (surprising, funny, shocking, heartwarming)\n",
    "- Relatability to young urban lifestyle\n",
    "- Potential for reader engagement/comments\n",
    "\n",
    "**NEWS DATA:**\n",
    "{df_json}\n",
    "\n",
    "**REQUIRED OUTPUT:**\n",
    "Please provide exactly 10 stories in the following format:\n",
    "\n",
    "## TOP 5 NEWSWORTHY STORIES\n",
    "1. **[Original Source]**: [Title]\n",
    "   - **Why it matters to Swiss readers**: [Brief explanation]\n",
    "   - **Key angle for 20min**: [How to present it]\n",
    "\n",
    "2-5. [Continue same format]\n",
    "\n",
    "## TOP 5 TALKING PIECES\n",
    "1. **[Original Source]**: [Title]\n",
    "   - **Conversation starter**: [What makes this discussable]\n",
    "   - **20min angle**: [How to make it engaging]\n",
    "\n",
    "2-5. [Continue same format]\n",
    "\n",
    "Remember: 20min.ch readers want quick, impactful stories they can discuss with friends or share on social media. Focus on human stories, surprising facts, and content that connects to Swiss life.\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Function to make the API call\n",
    "def analyze_news_for_20min(news_df):\n",
    "    \"\"\"\n",
    "    Send news DataFrame to Claude API and get recommendations for 20min.ch\n",
    "    \"\"\"\n",
    "    # Convert DataFrame to JSON for the prompt\n",
    "    df_json = news_df.to_json(orient='records', indent=2)\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = create_prompt(df_json)\n",
    "    \n",
    "    print(\"Sending request to Claude API...\")\n",
    "    print(f\"Analyzing {len(news_df)} articles from {news_df['source'].nunique()} sources...\")\n",
    "    \n",
    "    try:\n",
    "        # Make the API call\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",  # Using Sonnet 3.5\n",
    "            max_tokens=2000,\n",
    "            temperature=0.7,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract the response\n",
    "        analysis = response.content[0].text\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CLAUDE'S RECOMMENDATIONS FOR 20MIN.CH\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        print(analysis)\n",
    "        \n",
    "        # Save the analysis\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        filename = f\"20min_analysis_{timestamp}.txt\"\n",
    "        \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Analysis generated at: {datetime.now()}\\n\")\n",
    "            f.write(f\"Total articles analyzed: {len(news_df)}\\n\")\n",
    "            f.write(f\"Sources: {news_df['source'].nunique()}\\n\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"RECOMMENDATIONS\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            f.write(analysis)\n",
    "        \n",
    "        print(f\"\\nüíæ Analysis saved to: {filename}\")\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error calling Claude API: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run the analysis\n",
    "print(\"üöÄ Starting 20min.ch content analysis...\")\n",
    "print(f\"üìä DataFrame contains {len(news_df)} articles\")\n",
    "\n",
    "# Call the function with your news DataFrame\n",
    "analysis_result = analyze_news_for_20min(news_df)\n",
    "\n",
    "# Optional: Create a summary DataFrame of recommended articles\n",
    "if analysis_result:\n",
    "    print(\"\\n‚úÖ Analysis complete! Check the output above for Claude's recommendations.\")\n",
    "    print(\"\\nTip: You can copy the recommended titles and search for them in your original DataFrame:\")\n",
    "    print(\"Example: news_df[news_df['title'].str.contains('search_term', case=False)]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27deb46d-1c85-4ec1-ba9b-5a79482d0769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Sending data to Zapier...\n",
      "‚úÖ Successfully sent to Zapier!\n",
      "üìß Email will be sent to: tom.vaillant@20minuten.ch\n",
      "üìÑ Document title: 20min.ch News Analysis - 05.06.2025\n",
      "\n",
      "üéØ Next steps in Zapier:\n",
      "1. Google Doc will be created automatically\n",
      "2. Email will be sent with the doc link\n",
      "\n",
      "üéâ All done! Check your Zapier dashboard to monitor the workflow.\n"
     ]
    }
   ],
   "source": [
    "def send_to_zapier(analysis_text, news_df):\n",
    "    \"\"\"\n",
    "    Send the formatted data to Zapier webhook\n",
    "    \"\"\"\n",
    "    print(\"üì§ Sending data to Zapier...\")\n",
    "    \n",
    "    # Prepare the payload - now with HTML as primary format\n",
    "    payload = {\n",
    "        \"date\": datetime.now().strftime(\"%d.%m.%Y\"),\n",
    "        \"time\": datetime.now().strftime(\"%H:%M CET\"),\n",
    "        \"document_title\": f\"20min.ch News Analysis - {datetime.now().strftime('%d.%m.%Y')}\",\n",
    "        \"document_content_html\": format_for_google_docs(analysis_text, news_df),\n",
    "        \"email_content_html\": create_email_html(analysis_text, news_df),\n",
    "        \"stats\": {\n",
    "            \"total_articles\": len(news_df),\n",
    "            \"total_sources\": news_df['source'].nunique(),\n",
    "            \"continents\": news_df['continent'].nunique()\n",
    "        },\n",
    "        \"recipient_email\": \"tom.vaillant@20minuten.ch\",\n",
    "        \"email_subject\": f\"Daily News Analysis - {datetime.now().strftime('%d.%m.%Y')}\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send to Zapier\n",
    "        response = requests.post(ZAPIER_WEBHOOK_URL, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Successfully sent to Zapier!\")\n",
    "            print(f\"üìß Email will be sent to: {payload['recipient_email']}\")\n",
    "            print(f\"üìÑ Document title: {payload['document_title']}\")\n",
    "            print(\"\\nüéØ Next steps in Zapier:\")\n",
    "            print(\"1. Google Doc will be created with HTML formatting\")\n",
    "            print(\"2. Email will be sent with the doc link\")\n",
    "            print(\"\\nüí° In Zapier, use 'document_content_html' field for Google Docs\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Error sending to Zapier: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception occurred: {str(e)}\")\n",
    "        return False\n",
    "        \n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Your Zapier webhook URL - REPLACE WITH YOUR ACTUAL URL\n",
    "ZAPIER_WEBHOOK_URL = \"https://hooks.zapier.com/hooks/catch/23240089/2vphjoe/\"\n",
    "\n",
    "def format_for_google_docs(analysis_text, news_df):\n",
    "    \"\"\"\n",
    "    Format the analysis as clean, compact HTML for Google Docs\n",
    "    \"\"\"\n",
    "    # Start with a simple, compact header\n",
    "    html_content = f\"\"\"<h1>20MIN.CH DAILY NEWS ANALYSIS</h1>\n",
    "<h2>{datetime.now().strftime('%d %B %Y')}</h2>\n",
    "\n",
    "<h3>ANALYSIS SUMMARY</h3>\n",
    "<p>\n",
    "<strong>Generated:</strong> {datetime.now().strftime('%d.%m.%Y at %H:%M CET')}<br>\n",
    "<strong>Articles Analyzed:</strong> {len(news_df)}<br>\n",
    "<strong>News Sources:</strong> {news_df['source'].nunique()}<br>\n",
    "<strong>Continents Covered:</strong> {news_df['continent'].nunique()}\n",
    "</p>\n",
    "\n",
    "<hr>\n",
    "\"\"\"\n",
    "    \n",
    "    # Process the analysis text with minimal formatting\n",
    "    lines = analysis_text.split('\\n')\n",
    "    current_section = \"\"\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Handle main headers\n",
    "        if line.startswith('## '):\n",
    "            header = line.replace('## ', '').replace('**', '')\n",
    "            html_content += f'<h2>{header}</h2>\\n'\n",
    "            current_section = header\n",
    "        \n",
    "        # Handle story entries (numbered items)\n",
    "        elif line[0:2] in ['1.', '2.', '3.', '4.', '5.']:\n",
    "            # Parse the story line\n",
    "            parts = line.split('**')\n",
    "            if len(parts) >= 3:\n",
    "                number = parts[0].strip()\n",
    "                source = parts[1].strip()\n",
    "                title = parts[2].strip().lstrip(':').strip()\n",
    "                \n",
    "                # Simple, compact formatting\n",
    "                html_content += f'<p><strong>{number} {source}:</strong> {title}</p>\\n'\n",
    "        \n",
    "        # Handle bullet points\n",
    "        elif line.startswith('- '):\n",
    "            content = line[2:].strip()\n",
    "            \n",
    "            # Handle bold markers\n",
    "            if '**' in content:\n",
    "                parts = content.split('**')\n",
    "                formatted_content = \"\"\n",
    "                for i, part in enumerate(parts):\n",
    "                    if i % 2 == 1:\n",
    "                        formatted_content += f'<strong>{part}</strong>'\n",
    "                    else:\n",
    "                        formatted_content += part\n",
    "                content = formatted_content\n",
    "            \n",
    "            # Add as indented paragraph\n",
    "            if ':' in content:\n",
    "                label, value = content.split(':', 1)\n",
    "                html_content += f'<p style=\"margin-left: 20px;\">‚Ä¢ <strong>{label}:</strong>{value}</p>\\n'\n",
    "            else:\n",
    "                html_content += f'<p style=\"margin-left: 20px;\">‚Ä¢ {content}</p>\\n'\n",
    "    \n",
    "    # Add compact methodology section\n",
    "    html_content += \"\"\"\n",
    "<hr>\n",
    "<h3>METHODOLOGY</h3>\n",
    "<p>This analysis was generated by:</p>\n",
    "<p style=\"margin-left: 20px;\">\n",
    "‚Ä¢ Scanning RSS feeds from major news outlets across all continents<br>\n",
    "‚Ä¢ Collecting the latest \"\"\" + str(len(news_df)) + \"\"\" articles from \"\"\" + str(news_df['source'].nunique()) + \"\"\" sources<br>\n",
    "‚Ä¢ Using Claude AI to identify stories most relevant to 20min.ch's audience<br>\n",
    "‚Ä¢ Applying selection criteria: Swiss relevance, shareability, visual potential, emotional engagement, conversation potential\n",
    "</p>\n",
    "\n",
    "<h3>SOURCE DISTRIBUTION</h3>\n",
    "<p>\"\"\"\n",
    "    \n",
    "    # Add source statistics in a compact format\n",
    "    source_lines = []\n",
    "    for continent in news_df['continent'].value_counts().index:\n",
    "        count = len(news_df[news_df['continent'] == continent])\n",
    "        source_lines.append(f'<strong>{continent}:</strong> {count} articles')\n",
    "    \n",
    "    html_content += ' | '.join(source_lines)\n",
    "    \n",
    "    html_content += f\"\"\"\n",
    "</p>\n",
    "\n",
    "<hr>\n",
    "<p><em>Generated automatically via GitHub Actions at {datetime.now().strftime('%H:%M:%S CET')}</em></p>\n",
    "\"\"\"\n",
    "    \n",
    "    return html_content\n",
    "\n",
    "def create_email_html(analysis_text, news_df):\n",
    "    \"\"\"\n",
    "    Create HTML content for the email with summary\n",
    "    \"\"\"\n",
    "    # Extract top stories for email preview\n",
    "    stories_preview = extract_top_stories(analysis_text)\n",
    "    \n",
    "    html_content = f\"\"\"\n",
    "<div style=\"font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto;\">\n",
    "    <h2 style=\"color: #d32f2f;\">20min.ch Daily News Analysis</h2>\n",
    "    \n",
    "    <p>Hallo Tom,</p>\n",
    "    \n",
    "    <p>Your automated news analysis for <strong>{datetime.now().strftime('%d.%m.%Y')}</strong> is ready.</p>\n",
    "    \n",
    "    <div style=\"background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin: 20px 0;\">\n",
    "        <h3 style=\"margin-top: 0;\">Quick Stats:</h3>\n",
    "        <ul style=\"list-style-type: none; padding-left: 0;\">\n",
    "            <li>üìä Articles analyzed: {len(news_df)}</li>\n",
    "            <li>üì∞ News sources: {news_df['source'].nunique()}</li>\n",
    "            <li>üåç Continents covered: {news_df['continent'].nunique()}</li>\n",
    "            <li>‚è∞ Generated at: {datetime.now().strftime('%H:%M CET')}</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"margin: 20px 0;\">\n",
    "        <h3>Today's Highlights:</h3>\n",
    "        {stories_preview}\n",
    "    </div>\n",
    "    \n",
    "    <p style=\"margin-top: 30px;\">\n",
    "        <em>üí° The full analysis includes detailed recommendations for each story, \n",
    "        including Swiss relevance and suggested presentation angles.</em>\n",
    "    </p>\n",
    "    \n",
    "    <hr style=\"border: none; border-top: 1px solid #ddd; margin: 30px 0;\">\n",
    "    \n",
    "    <p style=\"color: #666; font-size: 12px;\">\n",
    "        This analysis was automatically generated using AI to identify stories \n",
    "        most relevant to 20min.ch's audience.\n",
    "    </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "    \n",
    "    return html_content\n",
    "\n",
    "def extract_top_stories(analysis_text):\n",
    "    \"\"\"\n",
    "    Extract the first 2 stories from each category for email preview\n",
    "    \"\"\"\n",
    "    preview_html = \"\"\n",
    "    lines = analysis_text.split('\\n')\n",
    "    \n",
    "    # Flags to track sections\n",
    "    in_newsworthy = False\n",
    "    in_talking = False\n",
    "    story_count = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        if \"TOP 5 NEWSWORTHY\" in line:\n",
    "            in_newsworthy = True\n",
    "            in_talking = False\n",
    "            story_count = 0\n",
    "            preview_html += \"<h4>üî• Top Newsworthy:</h4><ul>\"\n",
    "        elif \"TOP 5 TALKING\" in line:\n",
    "            in_newsworthy = False\n",
    "            in_talking = True\n",
    "            story_count = 0\n",
    "            if \"</ul>\" not in preview_html[-5:]:\n",
    "                preview_html += \"</ul>\"\n",
    "            preview_html += \"<h4>üí¨ Top Talking Pieces:</h4><ul>\"\n",
    "        elif (in_newsworthy or in_talking) and story_count < 2:\n",
    "            if line.strip().startswith(('1.', '2.')):\n",
    "                # Extract the title part\n",
    "                if '**' in line and ':' in line:\n",
    "                    parts = line.split('**')\n",
    "                    if len(parts) >= 3:\n",
    "                        source = parts[1]\n",
    "                        title = parts[2].split(':')[1].strip() if ':' in parts[2] else parts[2].strip()\n",
    "                        preview_html += f\"<li><strong>{source}</strong>: {title}</li>\"\n",
    "                        story_count += 1\n",
    "    \n",
    "    if \"</ul>\" not in preview_html[-5:]:\n",
    "        preview_html += \"</ul>\"\n",
    "    \n",
    "    return preview_html\n",
    "\n",
    "def send_to_zapier(analysis_text, news_df):\n",
    "    \"\"\"\n",
    "    Send the formatted data to Zapier webhook\n",
    "    \"\"\"\n",
    "    print(\"üì§ Sending data to Zapier...\")\n",
    "    \n",
    "    # Prepare the payload\n",
    "    payload = {\n",
    "        \"date\": datetime.now().strftime(\"%d.%m.%Y\"),\n",
    "        \"time\": datetime.now().strftime(\"%H:%M CET\"),\n",
    "        \"document_title\": f\"20min.ch News Analysis - {datetime.now().strftime('%d.%m.%Y')}\",\n",
    "        \"document_content\": format_for_google_docs(analysis_text, news_df),\n",
    "        \"email_content_html\": create_email_html(analysis_text, news_df),\n",
    "        \"stats\": {\n",
    "            \"total_articles\": len(news_df),\n",
    "            \"total_sources\": news_df['source'].nunique(),\n",
    "            \"continents\": news_df['continent'].nunique()\n",
    "        },\n",
    "        \"recipient_email\": \"tom.vaillant@20minuten.ch\",\n",
    "        \"email_subject\": f\"Daily News Analysis - {datetime.now().strftime('%d.%m.%Y')}\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send to Zapier\n",
    "        response = requests.post(ZAPIER_WEBHOOK_URL, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Successfully sent to Zapier!\")\n",
    "            print(f\"üìß Email will be sent to: {payload['recipient_email']}\")\n",
    "            print(f\"üìÑ Document title: {payload['document_title']}\")\n",
    "            print(\"\\nüéØ Next steps in Zapier:\")\n",
    "            print(\"1. Google Doc will be created automatically\")\n",
    "            print(\"2. Email will be sent with the doc link\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Error sending to Zapier: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception occurred: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Test the webhook with sample data (optional)\n",
    "def test_zapier_webhook():\n",
    "    \"\"\"\n",
    "    Test the Zapier webhook with minimal data\n",
    "    \"\"\"\n",
    "    test_payload = {\n",
    "        \"test\": True,\n",
    "        \"message\": \"Testing webhook connection\",\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    print(\"üß™ Testing Zapier webhook...\")\n",
    "    response = requests.post(ZAPIER_WEBHOOK_URL, json=test_payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"‚úÖ Webhook test successful!\")\n",
    "        print(\"Check your Zapier dashboard to see the test data\")\n",
    "    else:\n",
    "        print(f\"‚ùå Webhook test failed: {response.status_code}\")\n",
    "\n",
    "# Main execution for your Jupyter notebook\n",
    "if 'analysis_result' in globals() and 'news_df' in globals():\n",
    "    # Send the analysis to Zapier\n",
    "    success = send_to_zapier(analysis_result, news_df)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\nüéâ All done! Check your Zapier dashboard to monitor the workflow.\")\n",
    "else:\n",
    "    print(\"‚ùå No analysis results found. Please run the Claude analysis first.\")\n",
    "    print(\"\\nüí° To test your webhook connection, run:\")\n",
    "    print(\"test_zapier_webhook()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed19cd-ef91-479f-a1c3-47e36f494080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
