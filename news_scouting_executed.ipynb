{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3356e4b0-9c0e-48c6-84d2-4f1761b115b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T05:10:00.879622Z",
     "iopub.status.busy": "2025-07-02T05:10:00.879147Z",
     "iopub.status.idle": "2025-07-02T05:10:42.404867Z",
     "shell.execute_reply": "2025-07-02T05:10:42.404246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 45 news sources from news_sources.csv\n",
      "Starting news feed scanner...\n",
      "\n",
      "\n",
      "North America:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Americas..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching NPR News..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching The New York Times..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching CBC News Canada..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching The Guardian US..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Mexico News Daily..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Europe:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Europe..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching The Guardian UK..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Deutsche Welle English..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching France24 English..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Euronews..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching POLITICO Europe..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Swiss Info..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Blick..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Spiegel Online..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Die Zeit..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching S√ºddeutsche Zeitung..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Frankfurter Allgemeine..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Bild..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Blick..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Tagesschau..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Neue Z√ºrcher Zeitung..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Tages-Anzeiger..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Der Standard AT..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Kurier..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching ORF News..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Asia:\n",
      "----------------------------------------\n",
      "  Fetching Al Jazeera..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching The Japan Times..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching South China Morning Post..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching The Hindu India..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Times of India..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Africa:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Africa..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching AllAfrica..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Mail & Guardian SA..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching News24 South Africa..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Morocco World News..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "South America:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Latin America..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Buenos Aires Times..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching MercoPress..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Colombia Reports..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Oceania:\n",
      "----------------------------------------\n",
      "  Fetching ABC News Australia..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Sydney Morning Herald..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching The Guardian Australia..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Stuff.co.nz..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Radio New Zealand..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ‚úì (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Creating DataFrame...\n",
      "\n",
      "‚úÖ Success! Collected 450 articles from 44 sources\n",
      "üìä Articles per continent:\n",
      "  North America: 60\n",
      "  Europe: 200\n",
      "  Asia: 50\n",
      "  Africa: 50\n",
      "  South America: 40\n",
      "  Oceania: 50\n",
      "\n",
      "üìä Statistics:\n",
      "  Articles with URLs: 450\n",
      "  Articles with descriptions: 440\n",
      "\n",
      "‚ú® DataFrame stored in variable 'news_df' with 450 articles\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict\n",
    "\n",
    "# Load news sources from CSV file\n",
    "def load_news_sources_from_csv(csv_path='news_sources.csv'):\n",
    "    \"\"\"Load news sources from CSV file and organize by continent.\"\"\"\n",
    "    try:\n",
    "        sources_df = pd.read_csv(csv_path)\n",
    "        print(f\"‚úÖ Loaded {len(sources_df)} news sources from {csv_path}\")\n",
    "        \n",
    "        # Organize by continent\n",
    "        news_feeds = {}\n",
    "        for continent in sources_df['continent'].unique():\n",
    "            continent_sources = sources_df[sources_df['continent'] == continent]\n",
    "            news_feeds[continent] = []\n",
    "            \n",
    "            for _, row in continent_sources.iterrows():\n",
    "                news_feeds[continent].append({\n",
    "                    'name': row['name'],\n",
    "                    'url': row['url']\n",
    "                })\n",
    "        \n",
    "        return news_feeds\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå CSV file '{csv_path}' not found!\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading CSV: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load news feeds from CSV\n",
    "NEWS_FEEDS = load_news_sources_from_csv()\n",
    "\n",
    "def fetch_feed_titles(feed_url, feed_name, max_titles=10):\n",
    "    \"\"\"Extract titles, descriptions, and URLs from a feed with simple error handling.\"\"\"\n",
    "    titles = []\n",
    "    \n",
    "    try:\n",
    "        print(f\"  Fetching {feed_name}...\", end=\"\", flush=True)\n",
    "        response = requests.get(feed_url, timeout=10, headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        \n",
    "        # Parse with feedparser\n",
    "        feed = feedparser.parse(response.content)\n",
    "        \n",
    "        # Extract titles, descriptions, and article URLs\n",
    "        for i, entry in enumerate(feed.entries[:max_titles]):\n",
    "            if hasattr(entry, 'title'):\n",
    "                # Try multiple fields for description content\n",
    "                description = \"\"\n",
    "                if hasattr(entry, 'summary') and entry.summary:\n",
    "                    description = entry.summary.strip()\n",
    "                elif hasattr(entry, 'description') and entry.description:\n",
    "                    description = entry.description.strip()\n",
    "                elif hasattr(entry, 'content') and entry.content:\n",
    "                    # Some feeds use content field\n",
    "                    if isinstance(entry.content, list) and len(entry.content) > 0:\n",
    "                        description = entry.content[0].get('value', '').strip()\n",
    "                    else:\n",
    "                        description = str(entry.content).strip()\n",
    "                \n",
    "                # Get the article URL\n",
    "                article_url = getattr(entry, 'link', '')\n",
    "                \n",
    "                titles.append({\n",
    "                    'source': feed_name,\n",
    "                    'title': entry.title.strip(),\n",
    "                    'description': description,\n",
    "                    'url': article_url,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        print(f\" ‚úì ({len(titles)} articles)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" ‚úó (Error: {type(e).__name__})\")\n",
    "    \n",
    "    return titles\n",
    "\n",
    "# Main execution\n",
    "print(\"Starting news feed scanner...\\n\")\n",
    "all_results = []\n",
    "\n",
    "# Process each continent\n",
    "for continent, feeds in NEWS_FEEDS.items():\n",
    "    print(f\"\\n{continent}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for feed in feeds:\n",
    "        titles = fetch_feed_titles(feed['url'], feed['name'])\n",
    "        \n",
    "        # Add continent info to each title\n",
    "        for title in titles:\n",
    "            title['continent'] = continent\n",
    "            title['feed_url'] = feed['url']\n",
    "            all_results.append(title)\n",
    "        \n",
    "        # Small delay to be respectful\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Create DataFrame\n",
    "print(f\"\\n\\nCreating DataFrame...\")\n",
    "news_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Show summary\n",
    "if not news_df.empty:\n",
    "    print(f\"\\n‚úÖ Success! Collected {len(news_df)} articles from {news_df['source'].nunique()} sources\")\n",
    "    print(f\"üìä Articles per continent:\")\n",
    "    for continent in news_df['continent'].unique():\n",
    "        count = len(news_df[news_df['continent'] == continent])\n",
    "        print(f\"  {continent}: {count}\")\n",
    "    \n",
    "    print(f\"\\nüìä Statistics:\")\n",
    "    print(f\"  Articles with URLs: {len(news_df[news_df['url'] != ''])}\")\n",
    "    print(f\"  Articles with descriptions: {len(news_df[news_df['description'] != ''])}\")\n",
    "else:\n",
    "    print(\"‚ùå No articles collected!\")\n",
    "\n",
    "print(f\"\\n‚ú® DataFrame stored in variable 'news_df' with {len(news_df)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad33a84-373b-4095-92ef-48a1223ef8d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T05:10:42.407135Z",
     "iopub.status.busy": "2025-07-02T05:10:42.406745Z",
     "iopub.status.idle": "2025-07-02T05:11:19.328075Z",
     "shell.execute_reply": "2025-07-02T05:11:19.327388Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded .env file from default location\n",
      "‚úÖ API Key loaded: sk-ant-api...vwAA\n",
      "üìä DataFrame contains 450 articles\n",
      "‚úÖ Loaded prompt template from analysis_prompt.txt\n",
      "üöÄ Analyzing 450 articles from 44 sources...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CLAUDE'S RECOMMENDATIONS FOR 20MIN.CH\n",
      "================================================================================\n",
      "\n",
      "## TOP 5 SCHWEIZ/LOKALE STORIES MIT MAXIMALER WIRKUNG\n",
      "\n",
      "1. **Blick**: \"Bauernhaus in Tagelswangen ZH abgebrannt ‚Äì Bewohnerin fl√ºchtete in letzter Sekunde: ¬´Ich habe alles verloren. Alles!¬ª\"\n",
      "   - **URL**: https://www.blick.ch/schweiz/zuerich/bauernhaus-in-tagelswangen-zh-abgebrannt-bewohnerin-fluechtete-in-letzter-sekunde-ich-habe-alles-verloren-alles-id21011583.html\n",
      "   - **Der Hook**: Dramatische Rettung in letzter Sekunde kombiniert mit totalem Verlust - die existenzielle Angst einer Nachbarin trifft direkt ins Herz. Jeder kann sich vorstellen, alles zu verlieren.\n",
      "\n",
      "2. **Blick**: \"Nur die Limmat macht Mirjana wieder blau\"\n",
      "   - **URL**: https://www.blick.ch/schweiz/blick-unterwegs-mit-der-waermebildkamera-nur-die-limmat-macht-mirjana-wieder-blau-id21012838.html\n",
      "   - **Der Hook**: Kreative Visualisierung der Hitze durch W√§rmebildkamera macht die aktuelle Hitzewelle greifbar. Pers√∂nliche Geschichte von Mirjana schafft Identifikation.\n",
      "\n",
      "3. **SRF**: \"Globaler Boom bei Klimaanlagen ‚Äì so sieht es in der Schweiz aus\"\n",
      "   - **URL**: https://www.srf.ch/news/wirtschaft/hitzetage-in-der-schweiz-globaler-boom-bei-klimaanlagen-so-sieht-es-in-der-schweiz-aus\n",
      "   - **Der Hook**: Verbindet aktuelle Hitze mit Zukunfts√§ngsten und wirtschaftlichen Fragen. Betrifft jeden Schweizer Haushalt direkt.\n",
      "\n",
      "4. **Blick**: \"Was steckt hinter der neuen Massnahme?: Matura-Pr√ºfungen: Gymnasium in Neuenburg testet Schummel-Detektor\"\n",
      "   - **URL**: https://www.blick.ch/schweiz/was-steckt-hinter-der-neuen-massnahme-matura-pruefungen-gymnasium-in-neuenburg-testet-schummel-detektor-id21012181.html\n",
      "   - **Der Hook**: Technologie vs. Tradition im Bildungswesen - weckt Neugier und Kontroverse. Relevant f√ºr Eltern, Sch√ºler und Lehrer.\n",
      "\n",
      "5. **NZZ**: \"Krokodil-Lokomotiven der Rh√§tischen Bahn fahren bald wieder\"\n",
      "   - **URL**: https://www.nzz.ch/schweiz/krokodil-lokomotiven-der-rhaetischen-bahn-fahren-bald-wieder-ld.1891606\n",
      "   - **Der Hook**: Nostalgie trifft Schweizer Eisenbahn-Romantik. Die Wiederbelebung eines nationalen Kulturguts ber√ºhrt den Stolz.\n",
      "\n",
      "## TOP 5 INTERNATIONALE STORIES MIT NARRATIVER KRAFT\n",
      "\n",
      "1. **BBC News Americas**: \"Trump visits 'Alligator Alcatraz', the next step in his immigration crackdown\"\n",
      "   - **URL**: https://www.bbc.com/news/articles/cm2zzdmrd9qo\n",
      "   - **Warum es funktioniert**: Der dramatische Name \"Alligator Alcatraz\" und die surreale Vorstellung eines Gef√§ngnisses inmitten von Alligatoren erzeugt filmreife Bilder. Kombiniert Bedrohung und Faszination.\n",
      "\n",
      "2. **BBC News Americas**: \"Sean 'Diddy' Combs jury to keep deliberating after deadlocking on most serious charge\"\n",
      "   - **URL**: https://www.bbc.com/news/articles/c20nn0p9xg2o\n",
      "   - **Warum es funktioniert**: Der Fall vom H√∂henflug zum Tiefpunkt - ein klassischer Sturz der M√§chtigen. Die Spannung des ungewissen Ausgangs h√§lt Leser gefesselt.\n",
      "\n",
      "3. **The Guardian**: \"Trump and Musk's feud blows up again with threats of Doge and deportation\"\n",
      "   - **URL**: https://www.theguardian.com/us-news/2025/jul/01/trump-musk-feud-doge-deportation\n",
      "   - **Warum es funktioniert**: Zwei der kontroversesten Pers√∂nlichkeiten unserer Zeit in einem epischen Machtkampf. David gegen Goliath im digitalen Zeitalter.\n",
      "\n",
      "4. **BBC News Asia**: \"North Korea opens massive beach resort, eyes Russian tourists\"\n",
      "   - **URL**: https://www.bbc.com/news/articles/cyvjj9lmq3zo\n",
      "   - **Warum es funktioniert**: Die bizarre Vorstellung eines nordkoreanischen \"Waikiki\" ist so surreal, dass sie fasziniert. Perfekte Mischung aus Absurdit√§t und geopolitischer Relevanz.\n",
      "\n",
      "5. **Al Jazeera**: \"US university bars trans athletes, erases records under pressure from Trump\"\n",
      "   - **URL**: https://www.aljazeera.com/news/2025/7/2/us-university-bans-trans-athletes-under-pressure-from-trump-administration\n",
      "   - **Warum es funktioniert**: Pers√∂nliche Schicksale treffen auf gesellschaftlichen Umbruch. Die L√∂schung von Rekorden symbolisiert das Ausl√∂schen von Identit√§ten.\n",
      "\n",
      "‚úÖ Analysis complete! Ready for Zapier push.\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from .env file\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    if Path('.env').exists():\n",
    "        load_dotenv('.env')\n",
    "        print(\"‚úÖ Loaded .env file\")\n",
    "    elif Path('../.env').exists():\n",
    "        load_dotenv('../.env')\n",
    "        print(\"‚úÖ Loaded .env file from parent directory\")\n",
    "    else:\n",
    "        load_dotenv()\n",
    "        print(\"‚úÖ Loaded .env file from default location\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"‚ùå python-dotenv not installed! Run: pip install python-dotenv\")\n",
    "    raise\n",
    "\n",
    "# Get API key from environment variable\n",
    "API_KEY = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "# Validate that we have the API key\n",
    "if not API_KEY:\n",
    "    print(\"‚ùå ANTHROPIC_API_KEY not found in environment variables!\")\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY environment variable not set!\")\n",
    "else:\n",
    "    # Show partial key for confirmation\n",
    "    masked_key = f\"{API_KEY[:10]}...{API_KEY[-4:]}\" if len(API_KEY) > 20 else \"KEY_TOO_SHORT\"\n",
    "    print(f\"‚úÖ API Key loaded: {masked_key}\")\n",
    "\n",
    "# Initialize the Claude client\n",
    "client = anthropic.Anthropic(api_key=API_KEY)\n",
    "\n",
    "def load_prompt_template(prompt_file='analysis_prompt.txt'):\n",
    "    \"\"\"Load the prompt template from external file.\"\"\"\n",
    "    try:\n",
    "        with open(prompt_file, 'r', encoding='utf-8') as f:\n",
    "            prompt_template = f.read()\n",
    "        print(f\"‚úÖ Loaded prompt template from {prompt_file}\")\n",
    "        return prompt_template\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Prompt file '{prompt_file}' not found!\")\n",
    "        raise\n",
    "\n",
    "def create_prompt(df_json):\n",
    "    prompt_template = load_prompt_template()\n",
    "    prompt = prompt_template.format(df_json=df_json)\n",
    "    return prompt\n",
    "\n",
    "def analyze_news_for_20min(news_df):\n",
    "    \"\"\"Send news DataFrame to Claude API and get recommendations for 20min.ch\"\"\"\n",
    "    # Convert DataFrame to JSON for the prompt\n",
    "    df_json = news_df.to_json(orient='records', indent=2)\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = create_prompt(df_json)\n",
    "    \n",
    "    print(f\"üöÄ Analyzing {len(news_df)} articles from {news_df['source'].nunique()} sources...\")\n",
    "    \n",
    "    try:\n",
    "        # Make the API call\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=2000,\n",
    "            temperature=0.7,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract the response\n",
    "        analysis = response.content[0].text\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CLAUDE'S RECOMMENDATIONS FOR 20MIN.CH\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        print(analysis)\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error calling Claude API: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run the analysis\n",
    "print(f\"üìä DataFrame contains {len(news_df)} articles\")\n",
    "analysis_result = analyze_news_for_20min(news_df)\n",
    "\n",
    "if analysis_result:\n",
    "    print(\"\\n‚úÖ Analysis complete! Ready for Zapier push.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27deb46d-1c85-4ec1-ba9b-5a79482d0769",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T05:11:19.330439Z",
     "iopub.status.busy": "2025-07-02T05:11:19.330051Z",
     "iopub.status.idle": "2025-07-02T05:11:19.469718Z",
     "shell.execute_reply": "2025-07-02T05:11:19.469058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded .env file\n",
      "\n",
      "üì§ Sending to Zapier...\n",
      "\n",
      "üîç Parsing analysis text...\n",
      "üîó Found URL for Blick: https://www.blick.ch/schweiz/zuerich/bauernhaus-in-tagelswan...\n",
      "üîó Found URL for Blick: https://www.blick.ch/schweiz/blick-unterwegs-mit-der-waermeb...\n",
      "üîó Found URL for SRF: https://www.srf.ch/news/wirtschaft/hitzetage-in-der-schweiz-...\n",
      "üîó Found URL for Blick: https://www.blick.ch/schweiz/was-steckt-hinter-der-neuen-mas...\n",
      "üîó Found URL for NZZ: https://www.nzz.ch/schweiz/krokodil-lokomotiven-der-rhaetisc...\n",
      "üîó Found URL for BBC News Americas: https://www.bbc.com/news/articles/cm2zzdmrd9qo...\n",
      "üîó Found URL for BBC News Americas: https://www.bbc.com/news/articles/c20nn0p9xg2o...\n",
      "üîó Found URL for The Guardian: https://www.theguardian.com/us-news/2025/jul/01/trump-musk-f...\n",
      "üîó Found URL for BBC News Asia: https://www.bbc.com/news/articles/cyvjj9lmq3zo...\n",
      "üîó Found URL for Al Jazeera: https://www.aljazeera.com/news/2025/7/2/us-university-bans-t...\n",
      "‚úÖ Parsed 2 sections with URLs and details\n",
      "\n",
      "üîç Parsing analysis text...\n",
      "üîó Found URL for Blick: https://www.blick.ch/schweiz/zuerich/bauernhaus-in-tagelswan...\n",
      "üîó Found URL for Blick: https://www.blick.ch/schweiz/blick-unterwegs-mit-der-waermeb...\n",
      "üîó Found URL for SRF: https://www.srf.ch/news/wirtschaft/hitzetage-in-der-schweiz-...\n",
      "üîó Found URL for Blick: https://www.blick.ch/schweiz/was-steckt-hinter-der-neuen-mas...\n",
      "üîó Found URL for NZZ: https://www.nzz.ch/schweiz/krokodil-lokomotiven-der-rhaetisc...\n",
      "üîó Found URL for BBC News Americas: https://www.bbc.com/news/articles/cm2zzdmrd9qo...\n",
      "üîó Found URL for BBC News Americas: https://www.bbc.com/news/articles/c20nn0p9xg2o...\n",
      "üîó Found URL for The Guardian: https://www.theguardian.com/us-news/2025/jul/01/trump-musk-f...\n",
      "üîó Found URL for BBC News Asia: https://www.bbc.com/news/articles/cyvjj9lmq3zo...\n",
      "üîó Found URL for Al Jazeera: https://www.aljazeera.com/news/2025/7/2/us-university-bans-t...\n",
      "‚úÖ Parsed 2 sections with URLs and details\n",
      "‚ùå Error sending to Zapier: 404\n",
      "Response: please unsubscribe me!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from jinja2 import Template\n",
    "\n",
    "# Load environment variables with dotenv if available (for local development)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"‚úÖ Loaded .env file\")\n",
    "except ImportError:\n",
    "    print(\"üìù dotenv not available - using system environment variables\")\n",
    "\n",
    "# Your Zapier webhook URL\n",
    "ZAPIER_WEBHOOK_URL = os.environ.get('ZAPIER_WEBHOOK_URL')\n",
    "\n",
    "# Add error checking\n",
    "if not ZAPIER_WEBHOOK_URL:\n",
    "    raise ValueError(\"ZAPIER_WEBHOOK_URL environment variable not set!\")\n",
    "\n",
    "def load_template(template_file):\n",
    "    \"\"\"Load HTML template from file.\"\"\"\n",
    "    try:\n",
    "        with open(template_file, 'r', encoding='utf-8') as f:\n",
    "            template_content = f.read()\n",
    "        return Template(template_content)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Template file '{template_file}' not found!\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading template file: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def parse_analysis_text(analysis_text):\n",
    "    \"\"\"Parse the Claude analysis text into structured data for templates.\"\"\"\n",
    "    print(\"\\nüîç Parsing analysis text...\")\n",
    "    \n",
    "    lines = analysis_text.split('\\n')\n",
    "    sections = []\n",
    "    current_section = None\n",
    "    current_story = None\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Handle section headers\n",
    "        if line.startswith('## '):\n",
    "            # Save previous section\n",
    "            if current_section and current_section['stories']:\n",
    "                sections.append(current_section)\n",
    "            \n",
    "            header = line.replace('## ', '').replace('**', '')\n",
    "            \n",
    "            # Determine section type and title\n",
    "            if \"SCHWEIZ/LOKALE STORIES\" in header or \"SWISS\" in header:\n",
    "                current_section = {\n",
    "                    'title': 'Schweiz/Lokale Stories mit maximaler Wirkung',\n",
    "                    'icon': 'üá®üá≠',\n",
    "                    'stories': []\n",
    "                }\n",
    "            elif \"INTERNATIONALE STORIES\" in header or \"INTERNATIONAL\" in header:\n",
    "                current_section = {\n",
    "                    'title': 'Internationale Stories mit narrativer Kraft', \n",
    "                    'icon': 'üåç',\n",
    "                    'stories': []\n",
    "                }\n",
    "            else:\n",
    "                current_section = {\n",
    "                    'title': header,\n",
    "                    'icon': '',\n",
    "                    'stories': []\n",
    "                }\n",
    "        \n",
    "        # Handle story entries\n",
    "        elif line[0:2] in ['1.', '2.', '3.', '4.', '5.'] and current_section:\n",
    "            # Save previous story\n",
    "            if current_story:\n",
    "                current_section['stories'].append(current_story)\n",
    "            \n",
    "            # Parse new story\n",
    "            parts = line.split('**')\n",
    "            if len(parts) >= 3:\n",
    "                number = parts[0].strip()\n",
    "                source = parts[1].strip()\n",
    "                title = parts[2].strip().lstrip(':').strip()\n",
    "                \n",
    "                # Remove quotes if present\n",
    "                if title.startswith('\"') and title.endswith('\"'):\n",
    "                    title = title[1:-1]\n",
    "                elif title.startswith('\"'):\n",
    "                    title = title[1:]\n",
    "                \n",
    "                current_story = {\n",
    "                    'number': number,\n",
    "                    'source': source,\n",
    "                    'title': title,\n",
    "                    'url': '',  # Will be populated from details\n",
    "                    'details': []  # Keep details structure for the labels\n",
    "                }\n",
    "        \n",
    "        # Handle detail bullet points - capture URL, Der Hook, or Warum es funktioniert\n",
    "        elif (line.startswith('- ') or line.startswith('   - ')) and current_story:\n",
    "            content = line.lstrip('- ').strip()\n",
    "            original_content = content  # Keep original for debugging\n",
    "            \n",
    "            # Check BEFORE processing bold markers - Updated to catch URL pattern\n",
    "            is_target_field = (\n",
    "                content.startswith('**URL**:') or\n",
    "                content.startswith('**Der Hook**:') or \n",
    "                content.startswith('**Warum es funktioniert**:') or\n",
    "                content.startswith('URL:') or\n",
    "                content.startswith('Der Hook:') or \n",
    "                content.startswith('Warum es funktioniert:')\n",
    "            )\n",
    "            \n",
    "            # Check for target fields\n",
    "            if is_target_field:\n",
    "                # Extract label and value from original content (without HTML)\n",
    "                clean_content = original_content.replace('**', '')  # Remove bold markers\n",
    "                \n",
    "                if ':' in clean_content:\n",
    "                    label, value = clean_content.split(':', 1)\n",
    "                    label = label.strip()\n",
    "                    value = value.strip()\n",
    "                    \n",
    "                    # Handle URL field specially\n",
    "                    if label == 'URL':\n",
    "                        current_story['url'] = value\n",
    "                        print(f\"üîó Found URL for {current_story['source']}: {value[:60]}...\")\n",
    "                    else:\n",
    "                        # Regular detail field (Der Hook or Warum es funktioniert)\n",
    "                        detail = {\n",
    "                            'label': label,\n",
    "                            'value': value\n",
    "                        }\n",
    "                        current_story['details'].append(detail)\n",
    "    \n",
    "    # Save final story and section\n",
    "    if current_story:\n",
    "        current_section['stories'].append(current_story)\n",
    "    if current_section and current_section['stories']:\n",
    "        sections.append(current_section)\n",
    "    \n",
    "    print(f\"‚úÖ Parsed {len(sections)} sections with URLs and details\")\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def format_for_google_docs(analysis_text, news_df):\n",
    "    \"\"\"Format the analysis using Jinja2 template for Google Docs.\"\"\"\n",
    "    template = load_template('doc_template.html')\n",
    "    sections = parse_analysis_text(analysis_text)\n",
    "    \n",
    "    # Prepare continent statistics\n",
    "    continent_names_de = {\n",
    "        'North America': 'Nordamerika',\n",
    "        'Europe': 'Europa', \n",
    "        'Asia': 'Asien',\n",
    "        'Africa': 'Afrika',\n",
    "        'South America': 'S√ºdamerika',\n",
    "        'Oceania': 'Ozeanien'\n",
    "    }\n",
    "    \n",
    "    continents = []\n",
    "    for continent in news_df['continent'].value_counts().index:\n",
    "        count = len(news_df[news_df['continent'] == continent])\n",
    "        continent_de = continent_names_de.get(continent, continent)\n",
    "        continents.append({'name': continent_de, 'count': count})\n",
    "    \n",
    "    # Render template\n",
    "    html_content = template.render(\n",
    "        date_formatted=datetime.now().strftime('%d. %B %Y'),\n",
    "        datetime_full=datetime.now().strftime('%d.%m.%Y um %H:%M CET'),\n",
    "        time_generated=datetime.now().strftime('%H:%M:%S CET'),\n",
    "        stats={\n",
    "            'total_articles': len(news_df),\n",
    "            'total_sources': news_df['source'].nunique(),\n",
    "            'continents': news_df['continent'].nunique()\n",
    "        },\n",
    "        sections=sections,\n",
    "        continents=continents\n",
    "    )\n",
    "    \n",
    "    return html_content\n",
    "\n",
    "def create_email_html(analysis_text, news_df):\n",
    "    \"\"\"Create HTML content for email using Jinja2 template.\"\"\"\n",
    "    template = load_template('email_template.html')\n",
    "    sections = parse_analysis_text(analysis_text)\n",
    "    \n",
    "    # Render template\n",
    "    html_content = template.render(\n",
    "        date=datetime.now().strftime('%d.%m.%Y'),\n",
    "        time=datetime.now().strftime('%H:%M CET'),\n",
    "        stats={\n",
    "            'total_articles': len(news_df),\n",
    "            'total_sources': news_df['source'].nunique(),\n",
    "            'continents': news_df['continent'].nunique()\n",
    "        },\n",
    "        sections=sections\n",
    "    )\n",
    "    \n",
    "    return html_content\n",
    "\n",
    "def send_to_zapier(analysis_text, news_df):\n",
    "    \"\"\"Send the formatted data to Zapier webhook\"\"\"\n",
    "    print(\"\\nüì§ Sending to Zapier...\")\n",
    "    \n",
    "    # Generate content\n",
    "    document_content = format_for_google_docs(analysis_text, news_df)\n",
    "    email_content = create_email_html(analysis_text, news_df)\n",
    "    \n",
    "    # Prepare the payload\n",
    "    payload = {\n",
    "        \"date\": datetime.now().strftime(\"%d.%m.%Y\"),\n",
    "        \"time\": datetime.now().strftime(\"%H:%M CET\"),\n",
    "        \"document_title\": f\"20min.ch News-Analyse - {datetime.now().strftime('%d.%m.%Y')}\",\n",
    "        \"document_content\": document_content,\n",
    "        \"email_content_html\": email_content,\n",
    "        \"stats\": {\n",
    "            \"total_articles\": len(news_df),\n",
    "            \"total_sources\": news_df['source'].nunique(),\n",
    "            \"continents\": news_df['continent'].nunique()\n",
    "        },\n",
    "        \"recipient_email\": \"tom.vaillant@20minuten.ch\",\n",
    "        \"email_subject\": f\"T√§gliche News-Analyse - {datetime.now().strftime('%d.%m.%Y')}\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send to Zapier\n",
    "        response = requests.post(ZAPIER_WEBHOOK_URL, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"‚úÖ Successfully sent to Zapier!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Error sending to Zapier: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception occurred: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Main execution for your Jupyter notebook\n",
    "if 'analysis_result' in globals() and 'news_df' in globals():\n",
    "    # Send the analysis to Zapier\n",
    "    success = send_to_zapier(analysis_result, news_df)\n",
    "    \n",
    "    if success:\n",
    "        print(\"üéâ All done! Check your Zapier dashboard.\")\n",
    "else:\n",
    "    print(\"‚ùå No analysis results found. Please run the Claude analysis first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "897df390-1878-420b-87c6-1da48d51b708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
