{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3356e4b0-9c0e-48c6-84d2-4f1761b115b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T11:54:02.435744Z",
     "iopub.status.busy": "2025-06-06T11:54:02.435270Z",
     "iopub.status.idle": "2025-06-06T11:54:43.270723Z",
     "shell.execute_reply": "2025-06-06T11:54:43.270155Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 45 news sources from news_sources.csv\n",
      "Starting news feed scanner...\n",
      "\n",
      "\n",
      "North America:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Americas..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching NPR News..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching The New York Times..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching CBC News Canada..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching The Guardian US..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Mexico News Daily..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Europe:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Europe..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching The Guardian UK..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Deutsche Welle English..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching France24 English..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Euronews..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching POLITICO Europe..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Swiss Info..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Blick..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (4 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Spiegel Online..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Die Zeit..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Süddeutsche Zeitung..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Frankfurter Allgemeine..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Bild..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Blick..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Tagesschau..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Neue Zürcher Zeitung..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Tages-Anzeiger..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Der Standard AT..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Kurier..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching ORF News..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Asia:\n",
      "----------------------------------------\n",
      "  Fetching Al Jazeera..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching The Japan Times..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching South China Morning Post..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching The Hindu India..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Times of India..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Africa:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Africa..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching AllAfrica..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Mail & Guardian SA..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching News24 South Africa..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Morocco World News..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (0 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "South America:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Latin America..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Buenos Aires Times..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching MercoPress..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Colombia Reports..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Oceania:\n",
      "----------------------------------------\n",
      "  Fetching ABC News Australia..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Sydney Morning Herald..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching The Guardian Australia..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Stuff.co.nz..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Fetching Radio New Zealand..."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ✓ (10 articles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Creating DataFrame...\n",
      "\n",
      "✅ Success! Collected 434 articles from 43 sources\n",
      "📊 Articles per continent:\n",
      "  North America: 60\n",
      "  Europe: 194\n",
      "  Asia: 50\n",
      "  Africa: 40\n",
      "  South America: 40\n",
      "  Oceania: 50\n",
      "\n",
      "📊 Statistics:\n",
      "  Articles with URLs: 434\n",
      "  Articles with descriptions: 425\n",
      "\n",
      "✨ DataFrame stored in variable 'news_df' with 434 articles\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict\n",
    "\n",
    "# Load news sources from CSV file\n",
    "def load_news_sources_from_csv(csv_path='news_sources.csv'):\n",
    "    \"\"\"Load news sources from CSV file and organize by continent.\"\"\"\n",
    "    try:\n",
    "        sources_df = pd.read_csv(csv_path)\n",
    "        print(f\"✅ Loaded {len(sources_df)} news sources from {csv_path}\")\n",
    "        \n",
    "        # Organize by continent\n",
    "        news_feeds = {}\n",
    "        for continent in sources_df['continent'].unique():\n",
    "            continent_sources = sources_df[sources_df['continent'] == continent]\n",
    "            news_feeds[continent] = []\n",
    "            \n",
    "            for _, row in continent_sources.iterrows():\n",
    "                news_feeds[continent].append({\n",
    "                    'name': row['name'],\n",
    "                    'url': row['url']\n",
    "                })\n",
    "        \n",
    "        return news_feeds\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ CSV file '{csv_path}' not found!\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading CSV: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load news feeds from CSV\n",
    "NEWS_FEEDS = load_news_sources_from_csv()\n",
    "\n",
    "def fetch_feed_titles(feed_url, feed_name, max_titles=10):\n",
    "    \"\"\"Extract titles, descriptions, and URLs from a feed with simple error handling.\"\"\"\n",
    "    titles = []\n",
    "    \n",
    "    try:\n",
    "        print(f\"  Fetching {feed_name}...\", end=\"\", flush=True)\n",
    "        response = requests.get(feed_url, timeout=10, headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        \n",
    "        # Parse with feedparser\n",
    "        feed = feedparser.parse(response.content)\n",
    "        \n",
    "        # Extract titles, descriptions, and article URLs\n",
    "        for i, entry in enumerate(feed.entries[:max_titles]):\n",
    "            if hasattr(entry, 'title'):\n",
    "                # Try multiple fields for description content\n",
    "                description = \"\"\n",
    "                if hasattr(entry, 'summary') and entry.summary:\n",
    "                    description = entry.summary.strip()\n",
    "                elif hasattr(entry, 'description') and entry.description:\n",
    "                    description = entry.description.strip()\n",
    "                elif hasattr(entry, 'content') and entry.content:\n",
    "                    # Some feeds use content field\n",
    "                    if isinstance(entry.content, list) and len(entry.content) > 0:\n",
    "                        description = entry.content[0].get('value', '').strip()\n",
    "                    else:\n",
    "                        description = str(entry.content).strip()\n",
    "                \n",
    "                # Get the article URL\n",
    "                article_url = getattr(entry, 'link', '')\n",
    "                \n",
    "                titles.append({\n",
    "                    'source': feed_name,\n",
    "                    'title': entry.title.strip(),\n",
    "                    'description': description,\n",
    "                    'url': article_url,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        print(f\" ✓ ({len(titles)} articles)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" ✗ (Error: {type(e).__name__})\")\n",
    "    \n",
    "    return titles\n",
    "\n",
    "# Main execution\n",
    "print(\"Starting news feed scanner...\\n\")\n",
    "all_results = []\n",
    "\n",
    "# Process each continent\n",
    "for continent, feeds in NEWS_FEEDS.items():\n",
    "    print(f\"\\n{continent}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for feed in feeds:\n",
    "        titles = fetch_feed_titles(feed['url'], feed['name'])\n",
    "        \n",
    "        # Add continent info to each title\n",
    "        for title in titles:\n",
    "            title['continent'] = continent\n",
    "            title['feed_url'] = feed['url']\n",
    "            all_results.append(title)\n",
    "        \n",
    "        # Small delay to be respectful\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Create DataFrame\n",
    "print(f\"\\n\\nCreating DataFrame...\")\n",
    "news_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Show summary\n",
    "if not news_df.empty:\n",
    "    print(f\"\\n✅ Success! Collected {len(news_df)} articles from {news_df['source'].nunique()} sources\")\n",
    "    print(f\"📊 Articles per continent:\")\n",
    "    for continent in news_df['continent'].unique():\n",
    "        count = len(news_df[news_df['continent'] == continent])\n",
    "        print(f\"  {continent}: {count}\")\n",
    "    \n",
    "    print(f\"\\n📊 Statistics:\")\n",
    "    print(f\"  Articles with URLs: {len(news_df[news_df['url'] != ''])}\")\n",
    "    print(f\"  Articles with descriptions: {len(news_df[news_df['description'] != ''])}\")\n",
    "else:\n",
    "    print(\"❌ No articles collected!\")\n",
    "\n",
    "print(f\"\\n✨ DataFrame stored in variable 'news_df' with {len(news_df)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bad33a84-373b-4095-92ef-48a1223ef8d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T11:54:43.272930Z",
     "iopub.status.busy": "2025-06-06T11:54:43.272729Z",
     "iopub.status.idle": "2025-06-06T11:55:21.005375Z",
     "shell.execute_reply": "2025-06-06T11:55:21.004706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded .env file from default location\n",
      "✅ API Key loaded: sk-ant-api...vwAA\n",
      "📊 DataFrame contains 434 articles\n",
      "✅ Loaded prompt template from analysis_prompt.txt\n",
      "🚀 Analyzing 434 articles from 43 sources...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CLAUDE'S RECOMMENDATIONS FOR 20MIN.CH\n",
      "================================================================================\n",
      "\n",
      "## TOP 5 SCHWEIZ/LOKALE STORIES MIT MAXIMALER WIRKUNG\n",
      "\n",
      "1. **Blick**: \"Anwohner Simon Staub (56) bekämpft US-Gigant: «Der neue McDonald's bringt uns nur Abfall, Lärm und Urin»\"\n",
      "   - **URL**: https://www.blick.ch/wirtschaft/anwohner-simon-staub-56-bekaempft-us-gigant-der-neue-mcdonalds-bringt-uns-nur-abfall-laerm-und-urin-id20938911.html\n",
      "   - **Der Hook**: David gegen Goliath am Limmatquai - der klassische Kampf zwischen lokalem Widerstand und globalem Konzern weckt Emotionen von Wut bis Mitgefühl. Jeder Zürcher kennt die Location.\n",
      "\n",
      "2. **Blick**: \"Brian Keller vor Gericht: Urteil zur Prügel-Attacke um 16 Uhr erwartet\"\n",
      "   - **URL**: https://www.blick.ch/schweiz/zuerich/prozess-wegen-pruegel-attacke-auf-skorp-brian-zeigt-keinerlei-reue-er-ist-ganz-klar-kein-opfer-id20937528.html\n",
      "   - **Der Hook**: Die endlose Saga um Brian - Schweizweit bekannt, polarisierend, mit allen Elementen eines Dramas: Gewalt, gescheiterte Resozialisierung, Reue versus Trotz.\n",
      "\n",
      "3. **Neue Zürcher Zeitung**: \"SNB-Präsident Schlegel unter Druck: Nun überwachen die USA die Schweiz wegen ihrer Währungspolitik\"\n",
      "   - **URL**: https://www.nzz.ch/wirtschaft/das-amerikanische-finanzministerium-hat-die-schweiz-auf-seine-ueberwachungsliste-gesetzt-ld.1887907\n",
      "   - **Der Hook**: Sorge um die wirtschaftliche Souveränität der Schweiz - berührt Stolz und Unabhängigkeitsgefühl, direkte Auswirkungen auf Franken-Kurs.\n",
      "\n",
      "4. **Swiss Info**: \"Rega-Center will in die Zentralschweiz ziehen\"\n",
      "   - **URL**: https://www.srf.ch/news/schweiz/rega-will-umziehen-rega-center-will-in-die-zentralschweiz-ziehen\n",
      "   - **Der Hook**: Die Rega - Schweizer Nationalstolz und Rettungsengel in einem. Der Umzug betrifft 200 Arbeitsplätze und weckt regionale Emotionen.\n",
      "\n",
      "5. **Blick**: \"Ex-Sondereinheitschef Markus Melzl zu Zürcher Razzia-Videos: «Bei Drogeneinsätzen klingelt die Polizei nicht zuerst an der Haustür»\"\n",
      "   - **URL**: https://www.blick.ch/schweiz/zuerich/ex-sondereinheit-chef-markus-melzl-zu-zuercher-razzia-videos-bei-drogeneinsaetzen-klingelt-die-polizei-nicht-zuerst-an-der-haustuer-id20938403.html\n",
      "   - **Der Hook**: Exklusive Einblicke in dramatische Polizeiaktion, verbindet Spannung eines Krimis mit lokaler Relevanz.\n",
      "\n",
      "## TOP 5 INTERNATIONALE STORIES MIT NARRATIVER KRAFT\n",
      "\n",
      "1. **BBC News Americas**: \"Trump and Musk trade insults as row erupts in public view\"\n",
      "   - **URL**: https://www.bbc.com/news/articles/c5yg98rl717o\n",
      "   - **Warum es funktioniert**: Titanenkampf zweier Egomanen - der reichste Mann der Welt gegen den mächtigsten. Drama, Verrat, persönliche Fehde mit globalen Konsequenzen.\n",
      "\n",
      "2. **The Guardian**: \"Death is not the end! From the new robot Walt Disney to Mountainhead, movies are fuelled by immortality\"\n",
      "   - **URL**: https://www.theguardian.com/film/2025/jun/06/death-is-not-the-end-from-the-new-robot-walt-disney-to-mountainhead-movies-are-fuelled-by-immortality\n",
      "   - **Warum es funktioniert**: Faszinierende Mischung aus Nostalgie, Zukunftsangst und ethischen Fragen - Walt Disney als KI-Zombie berührt universelle Ängste.\n",
      "\n",
      "3. **Al Jazeera**: \"Where does Poland's new president stand on Ukraine?\"\n",
      "   - **URL**: https://www.aljazeera.com/news/2025/6/6/where-does-polands-new-president-stand-on-ukraine\n",
      "   - **Warum es funktioniert**: Geopolitisches Drama mit persönlicher Note - zeigt, wie einzelne Führungspersönlichkeiten das Schicksal ganzer Nationen beeinflussen können.\n",
      "\n",
      "4. **South China Morning Post**: \"Ex-girlfriend who blackmailed K-pop idol with sex video gets suspended sentence\"\n",
      "   - **URL**: https://www.scmp.com/week-asia/lifestyle-culture/article/3313396/ex-girlfriend-who-blackmailed-k-pop-idol-sex-video-gets-suspended-sentence\n",
      "   - **Warum es funktioniert**: Sex, Macht, Erpressung und K-Pop - perfekte Mischung aus Prominenz, Skandal und moralischem Dilemma.\n",
      "\n",
      "5. **The Japan Times**: \"Strangers in Kyoto': Tea, tradition and passive-aggressive politeness\"\n",
      "   - **URL**: https://www.japantimes.co.jp/culture/2025/06/06/film/strangers-in-kyoto/\n",
      "   - **Warum es funktioniert**: Kulturclash-Komödie trifft auf universelle Themen wie Höflichkeit und soziale Normen, verpackt in faszinierendem Japan-Setting.\n",
      "\n",
      "✅ Analysis complete! Ready for Zapier push.\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from .env file\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    if Path('.env').exists():\n",
    "        load_dotenv('.env')\n",
    "        print(\"✅ Loaded .env file\")\n",
    "    elif Path('../.env').exists():\n",
    "        load_dotenv('../.env')\n",
    "        print(\"✅ Loaded .env file from parent directory\")\n",
    "    else:\n",
    "        load_dotenv()\n",
    "        print(\"✅ Loaded .env file from default location\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"❌ python-dotenv not installed! Run: pip install python-dotenv\")\n",
    "    raise\n",
    "\n",
    "# Get API key from environment variable\n",
    "API_KEY = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "# Validate that we have the API key\n",
    "if not API_KEY:\n",
    "    print(\"❌ ANTHROPIC_API_KEY not found in environment variables!\")\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY environment variable not set!\")\n",
    "else:\n",
    "    # Show partial key for confirmation\n",
    "    masked_key = f\"{API_KEY[:10]}...{API_KEY[-4:]}\" if len(API_KEY) > 20 else \"KEY_TOO_SHORT\"\n",
    "    print(f\"✅ API Key loaded: {masked_key}\")\n",
    "\n",
    "# Initialize the Claude client\n",
    "client = anthropic.Anthropic(api_key=API_KEY)\n",
    "\n",
    "def load_prompt_template(prompt_file='analysis_prompt.txt'):\n",
    "    \"\"\"Load the prompt template from external file.\"\"\"\n",
    "    try:\n",
    "        with open(prompt_file, 'r', encoding='utf-8') as f:\n",
    "            prompt_template = f.read()\n",
    "        print(f\"✅ Loaded prompt template from {prompt_file}\")\n",
    "        return prompt_template\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Prompt file '{prompt_file}' not found!\")\n",
    "        raise\n",
    "\n",
    "def create_prompt(df_json):\n",
    "    prompt_template = load_prompt_template()\n",
    "    prompt = prompt_template.format(df_json=df_json)\n",
    "    return prompt\n",
    "\n",
    "def analyze_news_for_20min(news_df):\n",
    "    \"\"\"Send news DataFrame to Claude API and get recommendations for 20min.ch\"\"\"\n",
    "    # Convert DataFrame to JSON for the prompt\n",
    "    df_json = news_df.to_json(orient='records', indent=2)\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = create_prompt(df_json)\n",
    "    \n",
    "    print(f\"🚀 Analyzing {len(news_df)} articles from {news_df['source'].nunique()} sources...\")\n",
    "    \n",
    "    try:\n",
    "        # Make the API call\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=2000,\n",
    "            temperature=0.7,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract the response\n",
    "        analysis = response.content[0].text\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CLAUDE'S RECOMMENDATIONS FOR 20MIN.CH\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        print(analysis)\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error calling Claude API: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run the analysis\n",
    "print(f\"📊 DataFrame contains {len(news_df)} articles\")\n",
    "analysis_result = analyze_news_for_20min(news_df)\n",
    "\n",
    "if analysis_result:\n",
    "    print(\"\\n✅ Analysis complete! Ready for Zapier push.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27deb46d-1c85-4ec1-ba9b-5a79482d0769",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-06T11:55:21.007495Z",
     "iopub.status.busy": "2025-06-06T11:55:21.007297Z",
     "iopub.status.idle": "2025-06-06T11:55:21.132992Z",
     "shell.execute_reply": "2025-06-06T11:55:21.132466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded .env file\n",
      "\n",
      "📤 Sending to Zapier...\n",
      "\n",
      "🔍 Parsing analysis text...\n",
      "🔗 Found URL for Blick: https://www.blick.ch/wirtschaft/anwohner-simon-staub-56-beka...\n",
      "🔗 Found URL for Blick: https://www.blick.ch/schweiz/zuerich/prozess-wegen-pruegel-a...\n",
      "🔗 Found URL for Neue Zürcher Zeitung: https://www.nzz.ch/wirtschaft/das-amerikanische-finanzminist...\n",
      "🔗 Found URL for Swiss Info: https://www.srf.ch/news/schweiz/rega-will-umziehen-rega-cent...\n",
      "🔗 Found URL for Blick: https://www.blick.ch/schweiz/zuerich/ex-sondereinheit-chef-m...\n",
      "🔗 Found URL for BBC News Americas: https://www.bbc.com/news/articles/c5yg98rl717o...\n",
      "🔗 Found URL for The Guardian: https://www.theguardian.com/film/2025/jun/06/death-is-not-th...\n",
      "🔗 Found URL for Al Jazeera: https://www.aljazeera.com/news/2025/6/6/where-does-polands-n...\n",
      "🔗 Found URL for South China Morning Post: https://www.scmp.com/week-asia/lifestyle-culture/article/331...\n",
      "🔗 Found URL for The Japan Times: https://www.japantimes.co.jp/culture/2025/06/06/film/strange...\n",
      "✅ Parsed 2 sections with URLs and details\n",
      "\n",
      "🔍 Parsing analysis text...\n",
      "🔗 Found URL for Blick: https://www.blick.ch/wirtschaft/anwohner-simon-staub-56-beka...\n",
      "🔗 Found URL for Blick: https://www.blick.ch/schweiz/zuerich/prozess-wegen-pruegel-a...\n",
      "🔗 Found URL for Neue Zürcher Zeitung: https://www.nzz.ch/wirtschaft/das-amerikanische-finanzminist...\n",
      "🔗 Found URL for Swiss Info: https://www.srf.ch/news/schweiz/rega-will-umziehen-rega-cent...\n",
      "🔗 Found URL for Blick: https://www.blick.ch/schweiz/zuerich/ex-sondereinheit-chef-m...\n",
      "🔗 Found URL for BBC News Americas: https://www.bbc.com/news/articles/c5yg98rl717o...\n",
      "🔗 Found URL for The Guardian: https://www.theguardian.com/film/2025/jun/06/death-is-not-th...\n",
      "🔗 Found URL for Al Jazeera: https://www.aljazeera.com/news/2025/6/6/where-does-polands-n...\n",
      "🔗 Found URL for South China Morning Post: https://www.scmp.com/week-asia/lifestyle-culture/article/331...\n",
      "🔗 Found URL for The Japan Times: https://www.japantimes.co.jp/culture/2025/06/06/film/strange...\n",
      "✅ Parsed 2 sections with URLs and details\n",
      "✅ Successfully sent to Zapier!\n",
      "🎉 All done! Check your Zapier dashboard.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from jinja2 import Template\n",
    "\n",
    "# Load environment variables with dotenv if available (for local development)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"✅ Loaded .env file\")\n",
    "except ImportError:\n",
    "    print(\"📝 dotenv not available - using system environment variables\")\n",
    "\n",
    "# Your Zapier webhook URL\n",
    "ZAPIER_WEBHOOK_URL = os.environ.get('ZAPIER_WEBHOOK_URL')\n",
    "\n",
    "# Add error checking\n",
    "if not ZAPIER_WEBHOOK_URL:\n",
    "    raise ValueError(\"ZAPIER_WEBHOOK_URL environment variable not set!\")\n",
    "\n",
    "def load_template(template_file):\n",
    "    \"\"\"Load HTML template from file.\"\"\"\n",
    "    try:\n",
    "        with open(template_file, 'r', encoding='utf-8') as f:\n",
    "            template_content = f.read()\n",
    "        return Template(template_content)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Template file '{template_file}' not found!\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading template file: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def parse_analysis_text(analysis_text):\n",
    "    \"\"\"Parse the Claude analysis text into structured data for templates.\"\"\"\n",
    "    print(\"\\n🔍 Parsing analysis text...\")\n",
    "    \n",
    "    lines = analysis_text.split('\\n')\n",
    "    sections = []\n",
    "    current_section = None\n",
    "    current_story = None\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        line = line.strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Handle section headers\n",
    "        if line.startswith('## '):\n",
    "            # Save previous section\n",
    "            if current_section and current_section['stories']:\n",
    "                sections.append(current_section)\n",
    "            \n",
    "            header = line.replace('## ', '').replace('**', '')\n",
    "            \n",
    "            # Determine section type and title\n",
    "            if \"SCHWEIZ/LOKALE STORIES\" in header or \"SWISS\" in header:\n",
    "                current_section = {\n",
    "                    'title': 'Schweiz/Lokale Stories mit maximaler Wirkung',\n",
    "                    'icon': '🇨🇭',\n",
    "                    'stories': []\n",
    "                }\n",
    "            elif \"INTERNATIONALE STORIES\" in header or \"INTERNATIONAL\" in header:\n",
    "                current_section = {\n",
    "                    'title': 'Internationale Stories mit narrativer Kraft', \n",
    "                    'icon': '🌍',\n",
    "                    'stories': []\n",
    "                }\n",
    "            else:\n",
    "                current_section = {\n",
    "                    'title': header,\n",
    "                    'icon': '',\n",
    "                    'stories': []\n",
    "                }\n",
    "        \n",
    "        # Handle story entries\n",
    "        elif line[0:2] in ['1.', '2.', '3.', '4.', '5.'] and current_section:\n",
    "            # Save previous story\n",
    "            if current_story:\n",
    "                current_section['stories'].append(current_story)\n",
    "            \n",
    "            # Parse new story\n",
    "            parts = line.split('**')\n",
    "            if len(parts) >= 3:\n",
    "                number = parts[0].strip()\n",
    "                source = parts[1].strip()\n",
    "                title = parts[2].strip().lstrip(':').strip()\n",
    "                \n",
    "                # Remove quotes if present\n",
    "                if title.startswith('\"') and title.endswith('\"'):\n",
    "                    title = title[1:-1]\n",
    "                elif title.startswith('\"'):\n",
    "                    title = title[1:]\n",
    "                \n",
    "                current_story = {\n",
    "                    'number': number,\n",
    "                    'source': source,\n",
    "                    'title': title,\n",
    "                    'url': '',  # Will be populated from details\n",
    "                    'details': []  # Keep details structure for the labels\n",
    "                }\n",
    "        \n",
    "        # Handle detail bullet points - capture URL, Der Hook, or Warum es funktioniert\n",
    "        elif (line.startswith('- ') or line.startswith('   - ')) and current_story:\n",
    "            content = line.lstrip('- ').strip()\n",
    "            original_content = content  # Keep original for debugging\n",
    "            \n",
    "            # Check BEFORE processing bold markers - Updated to catch URL pattern\n",
    "            is_target_field = (\n",
    "                content.startswith('**URL**:') or\n",
    "                content.startswith('**Der Hook**:') or \n",
    "                content.startswith('**Warum es funktioniert**:') or\n",
    "                content.startswith('URL:') or\n",
    "                content.startswith('Der Hook:') or \n",
    "                content.startswith('Warum es funktioniert:')\n",
    "            )\n",
    "            \n",
    "            # Check for target fields\n",
    "            if is_target_field:\n",
    "                # Extract label and value from original content (without HTML)\n",
    "                clean_content = original_content.replace('**', '')  # Remove bold markers\n",
    "                \n",
    "                if ':' in clean_content:\n",
    "                    label, value = clean_content.split(':', 1)\n",
    "                    label = label.strip()\n",
    "                    value = value.strip()\n",
    "                    \n",
    "                    # Handle URL field specially\n",
    "                    if label == 'URL':\n",
    "                        current_story['url'] = value\n",
    "                        print(f\"🔗 Found URL for {current_story['source']}: {value[:60]}...\")\n",
    "                    else:\n",
    "                        # Regular detail field (Der Hook or Warum es funktioniert)\n",
    "                        detail = {\n",
    "                            'label': label,\n",
    "                            'value': value\n",
    "                        }\n",
    "                        current_story['details'].append(detail)\n",
    "    \n",
    "    # Save final story and section\n",
    "    if current_story:\n",
    "        current_section['stories'].append(current_story)\n",
    "    if current_section and current_section['stories']:\n",
    "        sections.append(current_section)\n",
    "    \n",
    "    print(f\"✅ Parsed {len(sections)} sections with URLs and details\")\n",
    "    \n",
    "    return sections\n",
    "\n",
    "def format_for_google_docs(analysis_text, news_df):\n",
    "    \"\"\"Format the analysis using Jinja2 template for Google Docs.\"\"\"\n",
    "    template = load_template('doc_template.html')\n",
    "    sections = parse_analysis_text(analysis_text)\n",
    "    \n",
    "    # Prepare continent statistics\n",
    "    continent_names_de = {\n",
    "        'North America': 'Nordamerika',\n",
    "        'Europe': 'Europa', \n",
    "        'Asia': 'Asien',\n",
    "        'Africa': 'Afrika',\n",
    "        'South America': 'Südamerika',\n",
    "        'Oceania': 'Ozeanien'\n",
    "    }\n",
    "    \n",
    "    continents = []\n",
    "    for continent in news_df['continent'].value_counts().index:\n",
    "        count = len(news_df[news_df['continent'] == continent])\n",
    "        continent_de = continent_names_de.get(continent, continent)\n",
    "        continents.append({'name': continent_de, 'count': count})\n",
    "    \n",
    "    # Render template\n",
    "    html_content = template.render(\n",
    "        date_formatted=datetime.now().strftime('%d. %B %Y'),\n",
    "        datetime_full=datetime.now().strftime('%d.%m.%Y um %H:%M CET'),\n",
    "        time_generated=datetime.now().strftime('%H:%M:%S CET'),\n",
    "        stats={\n",
    "            'total_articles': len(news_df),\n",
    "            'total_sources': news_df['source'].nunique(),\n",
    "            'continents': news_df['continent'].nunique()\n",
    "        },\n",
    "        sections=sections,\n",
    "        continents=continents\n",
    "    )\n",
    "    \n",
    "    return html_content\n",
    "\n",
    "def create_email_html(analysis_text, news_df):\n",
    "    \"\"\"Create HTML content for email using Jinja2 template.\"\"\"\n",
    "    template = load_template('email_template.html')\n",
    "    sections = parse_analysis_text(analysis_text)\n",
    "    \n",
    "    # Render template\n",
    "    html_content = template.render(\n",
    "        date=datetime.now().strftime('%d.%m.%Y'),\n",
    "        time=datetime.now().strftime('%H:%M CET'),\n",
    "        stats={\n",
    "            'total_articles': len(news_df),\n",
    "            'total_sources': news_df['source'].nunique(),\n",
    "            'continents': news_df['continent'].nunique()\n",
    "        },\n",
    "        sections=sections\n",
    "    )\n",
    "    \n",
    "    return html_content\n",
    "\n",
    "def send_to_zapier(analysis_text, news_df):\n",
    "    \"\"\"Send the formatted data to Zapier webhook\"\"\"\n",
    "    print(\"\\n📤 Sending to Zapier...\")\n",
    "    \n",
    "    # Generate content\n",
    "    document_content = format_for_google_docs(analysis_text, news_df)\n",
    "    email_content = create_email_html(analysis_text, news_df)\n",
    "    \n",
    "    # Prepare the payload\n",
    "    payload = {\n",
    "        \"date\": datetime.now().strftime(\"%d.%m.%Y\"),\n",
    "        \"time\": datetime.now().strftime(\"%H:%M CET\"),\n",
    "        \"document_title\": f\"20min.ch News-Analyse - {datetime.now().strftime('%d.%m.%Y')}\",\n",
    "        \"document_content\": document_content,\n",
    "        \"email_content_html\": email_content,\n",
    "        \"stats\": {\n",
    "            \"total_articles\": len(news_df),\n",
    "            \"total_sources\": news_df['source'].nunique(),\n",
    "            \"continents\": news_df['continent'].nunique()\n",
    "        },\n",
    "        \"recipient_email\": \"tom.vaillant@20minuten.ch\",\n",
    "        \"email_subject\": f\"Tägliche News-Analyse - {datetime.now().strftime('%d.%m.%Y')}\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send to Zapier\n",
    "        response = requests.post(ZAPIER_WEBHOOK_URL, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"✅ Successfully sent to Zapier!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Error sending to Zapier: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Exception occurred: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Main execution for your Jupyter notebook\n",
    "if 'analysis_result' in globals() and 'news_df' in globals():\n",
    "    # Send the analysis to Zapier\n",
    "    success = send_to_zapier(analysis_result, news_df)\n",
    "    \n",
    "    if success:\n",
    "        print(\"🎉 All done! Check your Zapier dashboard.\")\n",
    "else:\n",
    "    print(\"❌ No analysis results found. Please run the Claude analysis first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "897df390-1878-420b-87c6-1da48d51b708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
