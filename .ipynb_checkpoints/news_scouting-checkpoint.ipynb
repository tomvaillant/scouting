{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3356e4b0-9c0e-48c6-84d2-4f1761b115b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting news feed scanner...\n",
      "\n",
      "\n",
      "North America:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Americas... ✓ (10 titles)\n",
      "  Fetching NPR News... ✓ (10 titles)\n",
      "  Fetching The New York Times... ✓ (10 titles)\n",
      "  Fetching CBC News Canada... ✓ (10 titles)\n",
      "  Fetching The Guardian US... ✓ (10 titles)\n",
      "  Fetching Mexico News Daily... ✓ (10 titles)\n",
      "\n",
      "Europe:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Europe... ✓ (10 titles)\n",
      "  Fetching The Guardian UK... ✓ (10 titles)\n",
      "  Fetching Deutsche Welle English... ✓ (10 titles)\n",
      "  Fetching France24 English... ✓ (10 titles)\n",
      "  Fetching Euronews... ✓ (0 titles)\n",
      "  Fetching POLITICO Europe... ✓ (10 titles)\n",
      "  Fetching Swiss Info... ✓ (0 titles)\n",
      "  Fetching Blick... ✓ (4 titles)\n",
      "\n",
      "Asia:\n",
      "----------------------------------------\n",
      "  Fetching Al Jazeera... ✓ (10 titles)\n",
      "  Fetching The Japan Times... ✓ (10 titles)\n",
      "  Fetching South China Morning Post... ✓ (10 titles)\n",
      "  Fetching The Hindu India... ✓ (10 titles)\n",
      "  Fetching Times of India... ✓ (10 titles)\n",
      "  Fetching Arab News... ✓ (0 titles)\n",
      "\n",
      "Africa:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Africa... ✓ (10 titles)\n",
      "  Fetching AllAfrica... ✓ (10 titles)\n",
      "  Fetching Mail & Guardian SA... ✓ (10 titles)\n",
      "  Fetching News24 South Africa... ✓ (10 titles)\n",
      "  Fetching Morocco World News... ✓ (10 titles)\n",
      "\n",
      "South America:\n",
      "----------------------------------------\n",
      "  Fetching BBC News Latin America... ✓ (10 titles)\n",
      "  Fetching Buenos Aires Times... ✓ (10 titles)\n",
      "  Fetching MercoPress... ✓ (0 titles)\n",
      "  Fetching Colombia Reports... ✓ (10 titles)\n",
      "\n",
      "Oceania:\n",
      "----------------------------------------\n",
      "  Fetching ABC News Australia... ✓ (10 titles)\n",
      "  Fetching Sydney Morning Herald... ✓ (10 titles)\n",
      "  Fetching The Guardian Australia... ✓ (10 titles)\n",
      "  Fetching Stuff.co.nz... ✓ (10 titles)\n",
      "  Fetching Radio New Zealand... ✓ (10 titles)\n",
      "\n",
      "\n",
      "Creating DataFrame...\n",
      "\n",
      "✅ Success! Collected 294 articles from 30 sources\n",
      "\n",
      "Articles per continent:\n",
      "  North America: 60\n",
      "  Europe: 54\n",
      "  Asia: 50\n",
      "  Africa: 50\n",
      "  South America: 30\n",
      "  Oceania: 50\n",
      "\n",
      "📰 Sample headlines:\n",
      "================================================================================\n",
      "BBC News Americas: Why are these 12 countries on Trump's travel-ban list?...\n",
      "BBC News Americas: Trump's new ban dodges pitfalls faced by last attempt, experts say...\n",
      "BBC News Americas: Building housing indigenous art burns down in Canada's wildfires...\n",
      "BBC News Americas: Supreme Court rules for heterosexual woman in discrimination case...\n",
      "BBC News Americas: Tariffs prompt record plunge in US imports...\n",
      "BBC News Americas: Supreme Court rejects Mexico lawsuit against US gunmakers...\n",
      "BBC News Americas: Trump suspends foreign student visas at Harvard...\n",
      "BBC News Americas: Trump orders inquiry into Biden's actions, alleging 'cognitive decline'...\n",
      "BBC News Americas: Musk turns on Republicans - and gives Trump's big bill a harder path...\n",
      "BBC News Americas: What Merz wants from Trump showdown meeting...\n",
      "\n",
      "\n",
      "✨ DataFrame stored in variable 'news_df' with 294 articles\n"
     ]
    }
   ],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import time\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import List, Dict\n",
    "\n",
    "# News feeds organized by continent (excluding paid/API-required sources)\n",
    "NEWS_FEEDS = {\n",
    "    \"North America\": [\n",
    "        {\"name\": \"BBC News Americas\", \"url\": \"https://feeds.bbci.co.uk/news/world/us_and_canada/rss.xml\"},\n",
    "        {\"name\": \"NPR News\", \"url\": \"https://feeds.npr.org/1001/rss.xml\"},\n",
    "        {\"name\": \"The New York Times\", \"url\": \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\"},\n",
    "        {\"name\": \"CBC News Canada\", \"url\": \"https://rss.cbc.ca/lineup/topstories.xml\"},\n",
    "        {\"name\": \"The Guardian US\", \"url\": \"https://www.theguardian.com/us-news/rss\"},\n",
    "        {\"name\": \"Mexico News Daily\", \"url\": \"https://mexiconewsdaily.com/feed/\"}\n",
    "    ],\n",
    "    \"Europe\": [\n",
    "        {\"name\": \"BBC News Europe\", \"url\": \"https://feeds.bbci.co.uk/news/world/europe/rss.xml\"},\n",
    "        {\"name\": \"The Guardian UK\", \"url\": \"https://www.theguardian.com/uk/rss\"},\n",
    "        {\"name\": \"Deutsche Welle English\", \"url\": \"https://rss.dw.com/rdf/rss-en-all\"},\n",
    "        {\"name\": \"France24 English\", \"url\": \"https://www.france24.com/en/rss\"},\n",
    "        {\"name\": \"Euronews\", \"url\": \"https://www.euronews.com/rss\"},\n",
    "        {\"name\": \"POLITICO Europe\", \"url\": \"https://www.politico.eu/feed/\"},\n",
    "        {\"name\": \"Swiss Info\", \"url\": \"https://www.swissinfo.ch/eng/latest-news/rss\"},\n",
    "        {\"name\": \"Blick\", \"url\": \"https://www.blick.ch/news/rss.xml\"}\n",
    "    ],\n",
    "    \"Asia\": [\n",
    "        {\"name\": \"Al Jazeera\", \"url\": \"https://www.aljazeera.com/xml/rss/all.xml\"},\n",
    "        {\"name\": \"The Japan Times\", \"url\": \"https://www.japantimes.co.jp/feed/\"},\n",
    "        {\"name\": \"South China Morning Post\", \"url\": \"https://www.scmp.com/rss/91/feed\"},\n",
    "        {\"name\": \"The Hindu India\", \"url\": \"https://www.thehindu.com/news/national/feeder/default.rss\"},\n",
    "        {\"name\": \"Times of India\", \"url\": \"https://timesofindia.indiatimes.com/rssfeedstopstories.cms\"},\n",
    "        {\"name\": \"Arab News\", \"url\": \"https://www.arabnews.com/rss.xml\"}\n",
    "    ],\n",
    "    \"Africa\": [\n",
    "        {\"name\": \"BBC News Africa\", \"url\": \"https://feeds.bbci.co.uk/news/world/africa/rss.xml\"},\n",
    "        {\"name\": \"AllAfrica\", \"url\": \"https://allafrica.com/tools/headlines/rdf/latest/headlines.rdf\"},\n",
    "        {\"name\": \"Mail & Guardian SA\", \"url\": \"https://mg.co.za/feed/\"},\n",
    "        {\"name\": \"News24 South Africa\", \"url\": \"https://feeds.news24.com/articles/news24/TopStories/rss\"},\n",
    "        {\"name\": \"Morocco World News\", \"url\": \"https://www.moroccoworldnews.com/feed/\"}\n",
    "    ],\n",
    "    \"South America\": [\n",
    "        {\"name\": \"BBC News Latin America\", \"url\": \"https://feeds.bbci.co.uk/news/world/latin_america/rss.xml\"},\n",
    "        {\"name\": \"Buenos Aires Times\", \"url\": \"https://www.batimes.com.ar/feed\"},\n",
    "        {\"name\": \"MercoPress\", \"url\": \"https://en.mercopress.com/rss/v2/headlines\"},\n",
    "        {\"name\": \"Colombia Reports\", \"url\": \"https://colombiareports.com/feed/\"}\n",
    "    ],\n",
    "    \"Oceania\": [\n",
    "        {\"name\": \"ABC News Australia\", \"url\": \"https://www.abc.net.au/news/feed/2942460/rss.xml\"},\n",
    "        {\"name\": \"Sydney Morning Herald\", \"url\": \"https://www.smh.com.au/rss/feed.xml\"},\n",
    "        {\"name\": \"The Guardian Australia\", \"url\": \"https://www.theguardian.com/australia-news/rss\"},\n",
    "        {\"name\": \"Stuff.co.nz\", \"url\": \"https://www.stuff.co.nz/rss\"},\n",
    "        {\"name\": \"Radio New Zealand\", \"url\": \"https://www.rnz.co.nz/rss/national.xml\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "def fetch_feed_titles(feed_url, feed_name, max_titles=10):\n",
    "    \"\"\"Extract titles from a feed with simple error handling.\"\"\"\n",
    "    titles = []\n",
    "    \n",
    "    try:\n",
    "        # Fetch with timeout\n",
    "        print(f\"  Fetching {feed_name}...\", end=\"\", flush=True)\n",
    "        response = requests.get(feed_url, timeout=10, headers={\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "        \n",
    "        # Parse with feedparser\n",
    "        feed = feedparser.parse(response.content)\n",
    "        \n",
    "        # Extract titles\n",
    "        for i, entry in enumerate(feed.entries[:max_titles]):\n",
    "            if hasattr(entry, 'title'):\n",
    "                titles.append({\n",
    "                    'source': feed_name,\n",
    "                    'title': entry.title.strip(),\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                })\n",
    "        \n",
    "        print(f\" ✓ ({len(titles)} titles)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" ✗ (Error: {type(e).__name__})\")\n",
    "    \n",
    "    return titles\n",
    "\n",
    "# Main execution\n",
    "print(\"Starting news feed scanner...\\n\")\n",
    "all_results = []\n",
    "\n",
    "# Process each continent\n",
    "for continent, feeds in NEWS_FEEDS.items():\n",
    "    print(f\"\\n{continent}:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for feed in feeds:\n",
    "        titles = fetch_feed_titles(feed['url'], feed['name'])\n",
    "        \n",
    "        # Add continent info to each title\n",
    "        for title in titles:\n",
    "            title['continent'] = continent\n",
    "            title['feed_url'] = feed['url']\n",
    "            all_results.append(title)\n",
    "        \n",
    "        # Small delay to be respectful\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Create DataFrame\n",
    "print(f\"\\n\\nCreating DataFrame...\")\n",
    "news_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Show summary\n",
    "if not news_df.empty:\n",
    "    print(f\"\\n✅ Success! Collected {len(news_df)} articles from {news_df['source'].nunique()} sources\")\n",
    "    print(f\"\\nArticles per continent:\")\n",
    "    for continent in news_df['continent'].unique():\n",
    "        count = len(news_df[news_df['continent'] == continent])\n",
    "        print(f\"  {continent}: {count}\")\n",
    "    \n",
    "    print(\"\\n📰 Sample headlines:\")\n",
    "    print(\"=\" * 80)\n",
    "    for _, row in news_df.head(10).iterrows():\n",
    "        print(f\"{row['source']}: {row['title'][:100]}...\")\n",
    "else:\n",
    "    print(\"❌ No articles collected!\")\n",
    "\n",
    "# The DataFrame is now available as 'news_df'\n",
    "print(f\"\\n\\n✨ DataFrame stored in variable 'news_df' with {len(news_df)} articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bad33a84-373b-4095-92ef-48a1223ef8d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded .env file from current directory\n",
      "✅ API Key loaded successfully: sk-ant-api...vwAA\n",
      "🚀 Starting 20min.ch content analysis...\n",
      "📊 DataFrame contains 304 articles\n",
      "Sending request to Claude API...\n",
      "Analyzing 304 articles from 31 sources...\n",
      "\n",
      "================================================================================\n",
      "CLAUDE'S RECOMMENDATIONS FOR 20MIN.CH\n",
      "================================================================================\n",
      "\n",
      "Based on the target audience and selection criteria, here are the most suitable stories for 20min.ch:\n",
      "\n",
      "## TOP 5 NEWSWORTHY STORIES\n",
      "1. **[Deutsche Welle]**: \"NATO ministers back defense spending increase\"\n",
      "   - **Why it matters to Swiss readers**: Though not NATO members, Swiss defense spending and security alignment with European neighbors is a major ongoing debate\n",
      "   - **Key angle for 20min**: \"What NATO's new spending targets mean for Swiss security\"\n",
      "\n",
      "2. **[BBC News Europe]**: \"Eight injured on Ryanair flight hit by 'severe turbulence'\"\n",
      "   - **Why it matters to Swiss readers**: Many Swiss use Ryanair for European travel\n",
      "   - **Key angle for 20min**: \"Dramatic footage: Swiss passengers describe 'terrifying' Ryanair turbulence\"\n",
      "\n",
      "3. **[POLITICO Europe]**: \"Merz avoids a blowup in the Oval, but Trump goes his own way on Russia\"\n",
      "   - **Why it matters to Swiss readers**: German leadership's relationship with Trump impacts Swiss-EU relations\n",
      "   - **Key angle for 20min**: \"What German leader's Trump meeting means for Switzerland\"\n",
      "\n",
      "4. **[BBC News Europe]**: \"Pornhub pulls out of France over age verification law\"\n",
      "   - **Why it matters to Swiss readers**: Similar regulations being discussed in Switzerland\n",
      "   - **Key angle for 20min**: \"Could Pornhub also leave Switzerland? New laws being considered\"\n",
      "\n",
      "5. **[The New York Times]**: \"How ASML, a Key Supplier to the Chip Industry, Is Navigating Trump's Trade War\"\n",
      "   - **Why it matters to Swiss readers**: Impact on Swiss tech industry and global supply chains\n",
      "   - **Key angle for 20min**: \"How trade war affects Swiss tech companies\"\n",
      "\n",
      "## TOP 5 TALKING PIECES\n",
      "1. **[Blick]**: \"Mehrere Verletzte: Passagiere bei Flug an die Decke geschleudert\"\n",
      "   - **Conversation starter**: Everyone's worst flight nightmare\n",
      "   - **20min angle**: Interactive poll: \"What's your worst turbulence experience?\"\n",
      "\n",
      "2. **[CBC News Canada]**: \"I was a lifelong thrifter, committed to buying second-hand. Then I became addicted\"\n",
      "   - **Conversation starter**: Rising trend of second-hand shopping addiction among young Swiss\n",
      "   - **20min angle**: \"Are you a thrift-shopping addict? Take our quiz\"\n",
      "\n",
      "3. **[The Guardian UK]**: \"Judge threatens to remove Sean 'Diddy' Combs from court over facial gestures\"\n",
      "   - **Conversation starter**: Celebrity courtroom drama and appropriate behavior\n",
      "   - **20min angle**: \"Most dramatic Swiss courtroom moments\"\n",
      "\n",
      "4. **[Mexico News Daily]**: \"Two Baja California restaurants add to Mexico's harvest of Michelin stars\"\n",
      "   - **Conversation starter**: Swiss food scene and Michelin recognition\n",
      "   - **20min angle**: \"Which Swiss restaurant deserves the next Michelin star? Vote now!\"\n",
      "\n",
      "5. **[Times of India]**: \"AI killing jobs: Google AI CEO Hassabis has a tip; If I were a student..\"\n",
      "   - **Conversation starter**: AI impact on Swiss job market and education\n",
      "   - **20min angle**: \"Will AI take your job? Check our Swiss profession risk calculator\"\n",
      "\n",
      "These selections combine important news with engaging content that will resonate with young Swiss readers and generate social media sharing and discussion.\n",
      "\n",
      "✅ Analysis complete! Check the output above for Claude's recommendations.\n",
      "\n",
      "Tip: You can copy the recommended titles and search for them in your original DataFrame:\n",
      "Example: news_df[news_df['title'].str.contains('search_term', case=False)]\n"
     ]
    }
   ],
   "source": [
    "import anthropic\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Load environment variables from .env file\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    \n",
    "    # Try multiple possible locations for .env file\n",
    "    # First, try the current directory\n",
    "    if Path('.env').exists():\n",
    "        load_dotenv('.env')\n",
    "        print(\"✅ Loaded .env file from current directory\")\n",
    "    # Then try parent directory\n",
    "    elif Path('../.env').exists():\n",
    "        load_dotenv('../.env')\n",
    "        print(\"✅ Loaded .env file from parent directory\")\n",
    "    else:\n",
    "        load_dotenv()  # This will search for .env in default locations\n",
    "        print(\"✅ Loaded .env file from default location\")\n",
    "        \n",
    "except ImportError:\n",
    "    print(\"❌ python-dotenv not installed! Run: pip install python-dotenv\")\n",
    "    raise\n",
    "\n",
    "# Get API key from environment variable\n",
    "API_KEY = os.environ.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "# Validate that we have the API key\n",
    "if not API_KEY:\n",
    "    print(\"❌ ANTHROPIC_API_KEY not found in environment variables!\")\n",
    "    print(\"Available environment variables:\", list(os.environ.keys()))\n",
    "    print(\"\\nPlease ensure:\")\n",
    "    print(\"1. Your .env file is in the same directory as this notebook\")\n",
    "    print(\"2. The .env file contains: ANTHROPIC_API_KEY=your-key-here\")\n",
    "    print(\"3. You've installed python-dotenv: pip install python-dotenv\")\n",
    "    raise ValueError(\"ANTHROPIC_API_KEY environment variable not set!\")\n",
    "else:\n",
    "    # Show partial key for confirmation (security: only show first and last few characters)\n",
    "    masked_key = f\"{API_KEY[:10]}...{API_KEY[-4:]}\" if len(API_KEY) > 20 else \"KEY_TOO_SHORT\"\n",
    "    print(f\"✅ API Key loaded successfully: {masked_key}\")\n",
    "\n",
    "# Initialize the Claude client with your API key\n",
    "client = anthropic.Anthropic(api_key=API_KEY)\n",
    "\n",
    "# Craft the prompt based on 20min.ch's audience profile\n",
    "def create_prompt(df_json):\n",
    "    prompt = f\"\"\"You are a news editor for 20min.ch, Switzerland's leading free commuter tabloid with over 2 million readers. Your audience consists of:\n",
    "\n",
    "**TARGET AUDIENCE PROFILE:**\n",
    "- Young urban commuters (15-40 years old)\n",
    "- German-speaking Swiss citizens\n",
    "- Quick readers who consume news during their commute (average 20 minutes)\n",
    "- Prefer bite-sized, engaging content with visual appeal\n",
    "- Interested in local Swiss news, lifestyle, entertainment, and conversational stories\n",
    "- Enjoy interactive content, social media integration, and stories that spark discussion\n",
    "- Appreciate humor, human interest stories, and relatable content\n",
    "\n",
    "**YOUR TASK:**\n",
    "Analyze the following news articles from various international sources and identify the most suitable content for 20min.ch readers.\n",
    "\n",
    "**SELECTION CRITERIA:**\n",
    "1. **Newsworthy Stories**: Must be genuinely important, impact Swiss readers, or have global significance\n",
    "2. **Talking Pieces**: Stories that will generate conversation, debate, or emotional response among young Swiss readers\n",
    "\n",
    "Consider:\n",
    "- Swiss relevance or connection\n",
    "- Shareability on social media\n",
    "- Visual story potential\n",
    "- Emotional impact (surprising, funny, shocking, heartwarming)\n",
    "- Relatability to young urban lifestyle\n",
    "- Potential for reader engagement/comments\n",
    "\n",
    "**NEWS DATA:**\n",
    "{df_json}\n",
    "\n",
    "**REQUIRED OUTPUT:**\n",
    "Please provide exactly 10 stories in the following format:\n",
    "\n",
    "## TOP 5 NEWSWORTHY STORIES\n",
    "1. **[Original Source]**: [Title]\n",
    "   - **Why it matters to Swiss readers**: [Brief explanation]\n",
    "   - **Key angle for 20min**: [How to present it]\n",
    "\n",
    "2-5. [Continue same format]\n",
    "\n",
    "## TOP 5 TALKING PIECES\n",
    "1. **[Original Source]**: [Title]\n",
    "   - **Conversation starter**: [What makes this discussable]\n",
    "   - **20min angle**: [How to make it engaging]\n",
    "\n",
    "2-5. [Continue same format]\n",
    "\n",
    "Remember: 20min.ch readers want quick, impactful stories they can discuss with friends or share on social media. Focus on human stories, surprising facts, and content that connects to Swiss life.\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "# Function to make the API call\n",
    "def analyze_news_for_20min(news_df):\n",
    "    \"\"\"\n",
    "    Send news DataFrame to Claude API and get recommendations for 20min.ch\n",
    "    \"\"\"\n",
    "    # Convert DataFrame to JSON for the prompt\n",
    "    df_json = news_df.to_json(orient='records', indent=2)\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = create_prompt(df_json)\n",
    "    \n",
    "    print(\"Sending request to Claude API...\")\n",
    "    print(f\"Analyzing {len(news_df)} articles from {news_df['source'].nunique()} sources...\")\n",
    "    \n",
    "    try:\n",
    "        # Make the API call\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",  # Using Sonnet 3.5\n",
    "            max_tokens=2000,\n",
    "            temperature=0.7,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract the response\n",
    "        analysis = response.content[0].text\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"CLAUDE'S RECOMMENDATIONS FOR 20MIN.CH\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "        print(analysis)\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error calling Claude API: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Run the analysis\n",
    "print(\"🚀 Starting 20min.ch content analysis...\")\n",
    "print(f\"📊 DataFrame contains {len(news_df)} articles\")\n",
    "\n",
    "# Call the function with your news DataFrame\n",
    "analysis_result = analyze_news_for_20min(news_df)\n",
    "\n",
    "# Optional: Create a summary DataFrame of recommended articles\n",
    "if analysis_result:\n",
    "    print(\"\\n✅ Analysis complete! Check the output above for Claude's recommendations.\")\n",
    "    print(\"\\nTip: You can copy the recommended titles and search for them in your original DataFrame:\")\n",
    "    print(\"Example: news_df[news_df['title'].str.contains('search_term', case=False)]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27deb46d-1c85-4ec1-ba9b-5a79482d0769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded .env file\n",
      "📤 Sending data to Zapier...\n",
      "✅ Successfully sent to Zapier!\n",
      "📧 Email will be sent to: tom.vaillant@20minuten.ch\n",
      "📄 Document title: 20min.ch News-Analyse - 05.06.2025\n",
      "\n",
      "🎯 Next steps in Zapier:\n",
      "1. Google Doc will be created automatically\n",
      "2. Email will be sent with the doc link\n",
      "\n",
      "🎉 All done! Check your Zapier dashboard to monitor the workflow.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Load environment variables with dotenv if available (for local development)\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"✅ Loaded .env file\")\n",
    "except ImportError:\n",
    "    print(\"📝 dotenv not available - using system environment variables\")\n",
    "\n",
    "# Your Zapier webhook URL\n",
    "ZAPIER_WEBHOOK_URL = os.environ.get('ZAPIER_WEBHOOK_URL')\n",
    "\n",
    "# Add error checking\n",
    "if not ZAPIER_WEBHOOK_URL:\n",
    "    raise ValueError(\"ZAPIER_WEBHOOK_URL environment variable not set!\")\n",
    "\n",
    "def format_for_google_docs(analysis_text, news_df):\n",
    "    \"\"\"\n",
    "    Format the analysis as clean, compact HTML for Google Docs in German\n",
    "    \"\"\"\n",
    "    # Start with a simple, compact header in German\n",
    "    html_content = f\"\"\"<h1>20MIN.CH TÄGLICHE NEWS-ANALYSE</h1>\n",
    "<h2>{datetime.now().strftime('%d. %B %Y')}</h2>\n",
    "\n",
    "<h3>ANALYSE-ZUSAMMENFASSUNG</h3>\n",
    "<p>\n",
    "<strong>Erstellt:</strong> {datetime.now().strftime('%d.%m.%Y um %H:%M CET')}<br>\n",
    "<strong>Analysierte Artikel:</strong> {len(news_df)}<br>\n",
    "<strong>Nachrichtenquellen:</strong> {news_df['source'].nunique()}<br>\n",
    "<strong>Kontinente abgedeckt:</strong> {news_df['continent'].nunique()}\n",
    "</p>\n",
    "\n",
    "<hr>\n",
    "\"\"\"\n",
    "    \n",
    "    # Process the analysis text with minimal formatting\n",
    "    lines = analysis_text.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        \n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        # Handle main headers\n",
    "        if line.startswith('## '):\n",
    "            header = line.replace('## ', '').replace('**', '')\n",
    "            html_content += f'<h2>{header}</h2>\\n'\n",
    "        \n",
    "        # Handle story entries (numbered items) - keep original titles without translation\n",
    "        elif line[0:2] in ['1.', '2.', '3.', '4.', '5.']:\n",
    "            # Parse the story line\n",
    "            parts = line.split('**')\n",
    "            if len(parts) >= 3:\n",
    "                number = parts[0].strip()\n",
    "                source = parts[1].strip()\n",
    "                title = parts[2].strip().lstrip(':').strip()\n",
    "                \n",
    "                # Simple, compact formatting - keep original title\n",
    "                html_content += f'<p><strong>{number} {source}:</strong> {title}</p>\\n'\n",
    "        \n",
    "        # Handle bullet points with German labels\n",
    "        elif line.startswith('- '):\n",
    "            content = line[2:].strip()\n",
    "            \n",
    "            # Handle bold markers\n",
    "            if '**' in content:\n",
    "                parts = content.split('**')\n",
    "                formatted_content = \"\"\n",
    "                for i, part in enumerate(parts):\n",
    "                    if i % 2 == 1:\n",
    "                        formatted_content += f'<strong>{part}</strong>'\n",
    "                    else:\n",
    "                        formatted_content += part\n",
    "                content = formatted_content\n",
    "            \n",
    "            # Translate common English labels to German\n",
    "            content = content.replace('Why it matters to Swiss readers:', 'Warum es für Schweizer Leser wichtig ist:')\n",
    "            content = content.replace('Key angle for 20min:', '20min-Winkel:')\n",
    "            content = content.replace('Conversation starter:', 'Gesprächsanstoß:')\n",
    "            content = content.replace('20min angle:', '20min-Winkel:')\n",
    "            \n",
    "            # Add as indented paragraph\n",
    "            if ':' in content:\n",
    "                label, value = content.split(':', 1)\n",
    "                html_content += f'<p style=\"margin-left: 20px;\">• <strong>{label}:</strong>{value}</p>\\n'\n",
    "            else:\n",
    "                html_content += f'<p style=\"margin-left: 20px;\">• {content}</p>\\n'\n",
    "    \n",
    "    # Add compact methodology section in German\n",
    "    html_content += f\"\"\"\n",
    "<hr>\n",
    "<h3>METHODIK</h3>\n",
    "<p>Diese Analyse wurde erstellt durch:</p>\n",
    "<p style=\"margin-left: 20px;\">\n",
    "• Scannen von RSS-Feeds großer Nachrichtenportale aller Kontinente<br>\n",
    "• Sammeln der neuesten {len(news_df)} Artikel von {news_df['source'].nunique()} Quellen<br>\n",
    "• Verwendung von Claude AI zur Identifikation der für 20min.ch-Leser relevantesten Stories<br>\n",
    "• Anwendung von Auswahlkriterien: Schweiz-Relevanz, Teilbarkeit, visuelles Potenzial, emotionale Wirkung, Gesprächspotenzial\n",
    "</p>\n",
    "\n",
    "<h3>QUELLENVERTEILUNG</h3>\n",
    "<p>\"\"\"\n",
    "    \n",
    "    # Add source statistics in a compact format with German continent names\n",
    "    continent_names_de = {\n",
    "        'North America': 'Nordamerika',\n",
    "        'Europe': 'Europa', \n",
    "        'Asia': 'Asien',\n",
    "        'Africa': 'Afrika',\n",
    "        'South America': 'Südamerika',\n",
    "        'Oceania': 'Ozeanien'\n",
    "    }\n",
    "    \n",
    "    source_lines = []\n",
    "    for continent in news_df['continent'].value_counts().index:\n",
    "        count = len(news_df[news_df['continent'] == continent])\n",
    "        continent_de = continent_names_de.get(continent, continent)\n",
    "        source_lines.append(f'<strong>{continent_de}:</strong> {count} Artikel')\n",
    "    \n",
    "    html_content += ' | '.join(source_lines)\n",
    "    \n",
    "    html_content += f\"\"\"\n",
    "</p>\n",
    "\n",
    "<hr>\n",
    "<p><em>Automatisch generiert via GitHub Actions um {datetime.now().strftime('%H:%M:%S CET')}</em></p>\n",
    "\"\"\"\n",
    "    \n",
    "    return html_content\n",
    "\n",
    "def create_email_html(analysis_text, news_df):\n",
    "    \"\"\"\n",
    "    Create HTML content for the email with summary in German\n",
    "    \"\"\"\n",
    "    # Extract top stories for email preview\n",
    "    stories_preview = extract_top_stories(analysis_text)\n",
    "    \n",
    "    html_content = f\"\"\"\n",
    "<div style=\"font-family: Arial, sans-serif; max-width: 600px; margin: 0 auto;\">\n",
    "    <h2 style=\"color: #d32f2f;\">20min.ch Tägliche News-Analyse</h2>\n",
    "    \n",
    "    <p>Hallo Tom,</p>\n",
    "    \n",
    "    <p>Deine automatisierte News-Analyse für <strong>{datetime.now().strftime('%d.%m.%Y')}</strong> ist bereit.</p>\n",
    "    \n",
    "    <div style=\"background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin: 20px 0;\">\n",
    "        <h3 style=\"margin-top: 0;\">Schnellübersicht:</h3>\n",
    "        <ul style=\"list-style-type: none; padding-left: 0;\">\n",
    "            <li>📊 Analysierte Artikel: {len(news_df)}</li>\n",
    "            <li>📰 Nachrichtenquellen: {news_df['source'].nunique()}</li>\n",
    "            <li>🌍 Kontinente abgedeckt: {news_df['continent'].nunique()}</li>\n",
    "            <li>⏰ Erstellt um: {datetime.now().strftime('%H:%M CET')}</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "    \n",
    "    <div style=\"margin: 20px 0;\">\n",
    "        <h3>Heutige Empfehlungen:</h3>\n",
    "        {stories_preview}\n",
    "    </div>\n",
    "    \n",
    "    <p style=\"margin-top: 30px;\">\n",
    "        <em>💡 Die vollständige Analyse enthält detaillierte Empfehlungen für jede Story, \n",
    "        einschließlich Schweizer Relevanz und vorgeschlagener Präsentationswinkel.</em>\n",
    "    </p>\n",
    "    \n",
    "    <hr style=\"border: none; border-top: 1px solid #ddd; margin: 30px 0;\">\n",
    "    \n",
    "    <p style=\"color: #666; font-size: 12px;\">\n",
    "        Diese Analyse wurde automatisch mit KI erstellt, um die für das 20min.ch-Publikum \n",
    "        relevantesten Stories zu identifizieren.\n",
    "    </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "    \n",
    "    return html_content\n",
    "\n",
    "def extract_top_stories(analysis_text):\n",
    "    \"\"\"\n",
    "    Extract all 5 stories from each category for email preview\n",
    "    \"\"\"\n",
    "    preview_html = \"\"\n",
    "    lines = analysis_text.split('\\n')\n",
    "    \n",
    "    # Flags to track sections\n",
    "    in_newsworthy = False\n",
    "    in_talking = False\n",
    "    story_count = 0\n",
    "    \n",
    "    for line in lines:\n",
    "        if \"TOP 5 NEWSWORTHY\" in line:\n",
    "            in_newsworthy = True\n",
    "            in_talking = False\n",
    "            story_count = 0\n",
    "            preview_html += \"<h4>🔥 Top 5 Newsworthy Stories:</h4><ul>\"\n",
    "        elif \"TOP 5 TALKING\" in line:\n",
    "            in_newsworthy = False\n",
    "            in_talking = True\n",
    "            story_count = 0\n",
    "            if \"</ul>\" not in preview_html[-5:]:\n",
    "                preview_html += \"</ul>\"\n",
    "            preview_html += \"<h4>💬 Top 5 Talking Pieces:</h4><ul>\"\n",
    "        elif (in_newsworthy or in_talking) and story_count < 5:\n",
    "            if line.strip().startswith(('1.', '2.', '3.', '4.', '5.')):\n",
    "                # Extract the title part\n",
    "                if '**' in line and ':' in line:\n",
    "                    parts = line.split('**')\n",
    "                    if len(parts) >= 3:\n",
    "                        source = parts[1]\n",
    "                        title = parts[2].split(':')[1].strip() if ':' in parts[2] else parts[2].strip()\n",
    "                        preview_html += f\"<li><strong>{source}</strong>: {title}</li>\"\n",
    "                        story_count += 1\n",
    "    \n",
    "    if \"</ul>\" not in preview_html[-5:]:\n",
    "        preview_html += \"</ul>\"\n",
    "    \n",
    "    return preview_html\n",
    "\n",
    "def send_to_zapier(analysis_text, news_df):\n",
    "    \"\"\"\n",
    "    Send the formatted data to Zapier webhook\n",
    "    \"\"\"\n",
    "    print(\"📤 Sending data to Zapier...\")\n",
    "    \n",
    "    # Prepare the payload\n",
    "    payload = {\n",
    "        \"date\": datetime.now().strftime(\"%d.%m.%Y\"),\n",
    "        \"time\": datetime.now().strftime(\"%H:%M CET\"),\n",
    "        \"document_title\": f\"20min.ch News-Analyse - {datetime.now().strftime('%d.%m.%Y')}\",\n",
    "        \"document_content\": format_for_google_docs(analysis_text, news_df),\n",
    "        \"email_content_html\": create_email_html(analysis_text, news_df),\n",
    "        \"stats\": {\n",
    "            \"total_articles\": len(news_df),\n",
    "            \"total_sources\": news_df['source'].nunique(),\n",
    "            \"continents\": news_df['continent'].nunique()\n",
    "        },\n",
    "        \"recipient_email\": \"tom.vaillant@20minuten.ch\",\n",
    "        \"email_subject\": f\"Tägliche News-Analyse - {datetime.now().strftime('%d.%m.%Y')}\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Send to Zapier\n",
    "        response = requests.post(ZAPIER_WEBHOOK_URL, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            print(\"✅ Successfully sent to Zapier!\")\n",
    "            print(f\"📧 Email will be sent to: {payload['recipient_email']}\")\n",
    "            print(f\"📄 Document title: {payload['document_title']}\")\n",
    "            print(\"\\n🎯 Next steps in Zapier:\")\n",
    "            print(\"1. Google Doc will be created automatically\")\n",
    "            print(\"2. Email will be sent with the doc link\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Error sending to Zapier: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Exception occurred: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Test the webhook with sample data (optional)\n",
    "def test_zapier_webhook():\n",
    "    \"\"\"\n",
    "    Test the Zapier webhook with minimal data\n",
    "    \"\"\"\n",
    "    test_payload = {\n",
    "        \"test\": True,\n",
    "        \"message\": \"Testing webhook connection\",\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    print(\"🧪 Testing Zapier webhook...\")\n",
    "    response = requests.post(ZAPIER_WEBHOOK_URL, json=test_payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ Webhook test successful!\")\n",
    "        print(\"Check your Zapier dashboard to see the test data\")\n",
    "    else:\n",
    "        print(f\"❌ Webhook test failed: {response.status_code}\")\n",
    "\n",
    "# Main execution for your Jupyter notebook\n",
    "if 'analysis_result' in globals() and 'news_df' in globals():\n",
    "    # Send the analysis to Zapier\n",
    "    success = send_to_zapier(analysis_result, news_df)\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n🎉 All done! Check your Zapier dashboard to monitor the workflow.\")\n",
    "else:\n",
    "    print(\"❌ No analysis results found. Please run the Claude analysis first.\")\n",
    "    print(\"\\n💡 To test your webhook connection, run:\")\n",
    "    print(\"test_zapier_webhook()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ed19cd-ef91-479f-a1c3-47e36f494080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
